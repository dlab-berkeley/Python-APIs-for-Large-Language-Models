{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5dd108-8773-4c13-8e02-87b3569b9734",
   "metadata": {},
   "source": [
    "# Interfacing Directly with LLMs \n",
    "### (Taking the Web Interface Out of the Picture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a540d-da5a-4c4a-ae3f-8d9db813f313",
   "metadata": {},
   "source": [
    "<img src=\"../images/llm_api_access.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d233d4-1bd0-4d59-a79c-e1fc67424a6a",
   "metadata": {},
   "source": [
    "In Part 1, we made our first API call using OpenRouter and saw how to connect to a model like DeepSeek using Python.\n",
    "Now that youâ€™ve made your first call to a model, letâ€™s take a closer look at one of the most common ways to talk to modern LLMs: the `/chat/completions` endpoint.\n",
    "\n",
    "Specfically in this section, you'll learn:\n",
    "- What inputs this endpoint expects\n",
    "- How the response is structured\n",
    "- How it compares to using ChatGPT interactively\n",
    "- Why this gives you more control and automation in research workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc09d5-2d60-447b-add1-fecb150a85d5",
   "metadata": {},
   "source": [
    "As a refresher, an API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate and interact with each other. It acts as an intermediary, defining how one piece of software can request services or data from another. Behind the scenes almost any website, resource, or software you are interacting with on the internet will use and define an API to connect, talk to, and transfer data between resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27bf2b-8b60-4ca3-8476-6b9c040c5b86",
   "metadata": {},
   "source": [
    "## Breaking down the `[POST] /chat/completions`?\n",
    "\n",
    "When you use ChatGPT or similar tools, the conversational experience you have is powered by an API endpoint called chat/completions. This endpoint is the engine behind a continuous conversation. You send it a **history of messages**, and it responds with the **next message in the conversation**.\n",
    "\n",
    "Normally, the service handles all of this for you. But in this workshop, we'll get into the specifics of how to build and manage these API requests yourself. \n",
    "\n",
    "### Building a ChatCompletions Request\n",
    "When you send a request to the chat/completions endpoint, you're essentially providing the model with a list of messages. The model then generates a new message to add to that list.\n",
    "\n",
    "Your request needs to include two main components: the model you want to use and the conversation history itself.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello!\"\n",
    "      }\n",
    "    ],\n",
    "    ... optional parameters...\n",
    "}\n",
    "```\n",
    "Let's break down the key parts of this request:\n",
    "- `model` This specifies which large language model you want to use. You might choose a model like `gpt-4o` from OpenAI, `claude-3-opus` from Anthropic, or others.\n",
    "- `messages` [This is a list of all the messages in the conversation so far. Each message object has two parts]\n",
    "    - `role` [This defines who is \"speaking.\" We will dive more into this later.]\n",
    "    - `content` [This is the actual text of the message.] \n",
    "\n",
    "### Understanding the [`ChatCompletion`](https://platform.openai.com/docs/api-reference/chat/object) Response\n",
    "Once you send a request, the API returns a ChatCompletion object. This object contains the model's new message, along with a lot of other useful information\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1741570283,\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hey, how are you?\",\n",
    "        \"refusal\": null,\n",
    "        \"annotations\": []\n",
    "      },\n",
    "      \"logprobs\": null,\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 1117,\n",
    "    \"completion_tokens\": 46,\n",
    "    \"total_tokens\": 1163,\n",
    "    \"prompt_tokens_details\": {\n",
    "      \"cached_tokens\": 0,\n",
    "      \"audio_tokens\": 0\n",
    "    },\n",
    "    \"completion_tokens_details\": {\n",
    "      \"reasoning_tokens\": 0,\n",
    "      \"audio_tokens\": 0,\n",
    "      \"accepted_prediction_tokens\": 0,\n",
    "      \"rejected_prediction_tokens\": 0\n",
    "    }\n",
    "  },\n",
    "  \"service_tier\": \"default\",\n",
    "  \"system_fingerprint\": \"fp_fc9f1d7035\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "There's a lot of information here, but for now, you only need to focus on a few key parts:\n",
    "\n",
    "- `choices` [This is an array that contains the model's response. In most cases, you'll just be looking at the first (and only) item in this list]\n",
    "    - `message`: Inside choices, this is the new message object.\n",
    "        - `role`: This will always be `assistant`, since it's the model's response.\n",
    "        - `content`: This is the actual text generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d4ad8-e084-4674-b58b-00ca944e0536",
   "metadata": {},
   "source": [
    "### Let's talk to our agent: A simple Chat Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb49561-2687-48dd-9b0b-31dac4d5f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, setup our Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Read the API_KEY\n",
    "with open('API_KEY.txt', 'r') as file:\n",
    "    API_KEY = file.read()\n",
    "    \n",
    "# Intialize Client\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\", \n",
    "  api_key=API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e4ffb-c223-4e68-8be1-2ff676c48c15",
   "metadata": {},
   "source": [
    "Next, we'll build our first conversation request. This is where the `messages` array comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134d511e-0353-4714-8e50-e58c5de0221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Sohail\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Hello! I am {<WHAT_GOES_HERE>}, nice to meet you!\"} # Fill in your name\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12744e8-0493-4bb4-918b-c75d1590fbfc",
   "metadata": {},
   "source": [
    "#### Instructor Notes: before you assume understanding and continue, stop here and ask people if they understand what just happened here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43502792-d3d9-48bd-81d5-123e2d113ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Sohail! Nice to meet you too! ðŸ˜Š How are you doing today? Is there anything I can help you with?\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820842d8-d642-4ccc-be31-71bca66d9d62",
   "metadata": {},
   "source": [
    "#### What's happening here?\n",
    "\n",
    "The `client.chat.completions.create()` method builds and sends the API request for us.\n",
    "\n",
    "- `model`: We're using a specific model available on OpenRouter, in this case, deepseek/deepseek-chat-v3-0324:free.\n",
    "- `messages`: This is our conversation history. We start with a system message to set the model's persona and a user message with our initial prompt. The f-string f\"Hello! I am {name}, nice to meet you!\" is a neat way to dynamically insert variables into your messages.\n",
    "- `print(completion.choices[0].message.content)`: This line shows how to parse the JSON response we talked about earlier. We access the first item in the choices list, then the message object, and finally the content field to get the text of the model's reply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448562e-da2e-4061-8c91-a4c3d6a6966a",
   "metadata": {},
   "source": [
    "## Simulating Memory with Message History\n",
    "A common misconception is that LLMs \"remember\" previous interactions. They don't. Each API request is entirely stateless, the model only knows what's in the messages list you send.\n",
    "\n",
    "To build a continuous conversation, you must simulate memory by including all prior messages in every new API request. Let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a7b11-a683-4db3-8d8f-db6f0dd4ae45",
   "metadata": {},
   "source": [
    "#### Example 1: Ask the Model To Remember Your Name\n",
    "In this first example, we include the entire conversation history in our request, so the model knows the user's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b1459a-09b2-44cd-acca-aa4904a55e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model to remember your name\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"My name is Sohail.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435d649c-a3dd-4d3b-8642-5766cde8e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is **Sohail**. ðŸ˜Š Let me know if there's anything I can assist you with, Sohail!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be6440-a529-4065-ae2a-a86866ab61b1",
   "metadata": {},
   "source": [
    "#### Example 2: The Model Forgets\n",
    "Now, what happens if we only send the final message? This is like starting a new chat in ChatGPT which has no history of your previous isolated conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c089c172-87ea-4e6a-b4df-6f3e6f3f1724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I donâ€™t have access to personal information about you, including your name, unless you share it with me. Let me know how I can assist you, and Iâ€™ll be happy to help! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# A new, isolated conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "# Send the request with only the last message\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# This will likely respond with something like \"I don't know your name.\"\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a48312-fadb-4074-90e6-fb0929752bba",
   "metadata": {},
   "source": [
    "---\n",
    "As you can see, without the \"My name is Sohail\" message, the model has no context. You are **responsible for managing** this message history yourself.\n",
    "\n",
    "ðŸ’¡\n",
    "LLMs donâ€™t have memory by default. If you want the model to remember something, you have to simulate memory by including prior messages in the messages list. In a platform like ChatGPT, this happens automatically: the UI handles the conversation history behind the scenes. But when working directly with the API, youâ€™re in charge of preserving that history yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733a11b-c153-4136-82c9-863139385cef",
   "metadata": {},
   "source": [
    "### A Deeper Look at the Roles: `user`, `assistant`, and `system`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77050a63-fa9e-4dc9-9f4b-d5d541bd22e9",
   "metadata": {},
   "source": [
    "By now, you've seen our messages list contain objects with different role values. The role is a crucial part of the API request; it tells the model who is speaking and provides essential context for its response.\n",
    "\n",
    "The three primary roles you will use are:\n",
    "- `user`: This represents **your** input - the prompt, the data, or the question you're giving the model. In our thematic coding scenario, for example, each essential worker's story would be placed within a user message.\n",
    "- `assistant`: This is the model's response. When the model generates a reply, its role is assistant. When building a multi-turn conversation, you'll take the model's response and add it back to the messages list with this role to preserve the conversation history.\n",
    "- `system`: This is a special, high-level role used to set the model's overall behavior, persona, or instructions before the conversation begins. Unlike user and assistant messages, the system message is not part of the back-and-forth chat; rather, it's a foundational set of rules that the model should follow throughout the entire interaction.\n",
    "\n",
    "Understanding these roles is key to building effective and reliable API calls. The system role, in particular, is an extremely powerful tool for a researcher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164605e3-e984-4ecd-a71a-c3f614a8a7a4",
   "metadata": {},
   "source": [
    "---\n",
    "Because the system message has a privileged position and often carries more weight than user messages, it is your primary tool for \"prompt engineering\" at a global level.\n",
    "\n",
    "A key benefit of the system role is its ability to enforce constraints and rules, ensuring consistent behavior across many data points. This is crucial for a research task like thematic coding.\n",
    "\n",
    "Let's use our COVID-19 narrative scenario to demonstrate this. We can use the system prompt to provide a strict set of instructions and a codebook for the model to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ca92cd-6cc2-44e8-9e03-d8f6cb6ef465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"emotions\": [\"fear\", \"sadness\", \"invisibility\"],\n",
      "  \"material_conditions\": [\"shortage of masks\", \"crowded subway\"],\n",
      "  \"solidarity_or_isolation\": \"isolation\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "research_prompt = \"\"\"\n",
    "You are an expert qualitative researcher assisting with a study on COVID-19 narratives.\n",
    "Your task is to analyze the following story and extract specific themes and details.\n",
    "\n",
    "Follow these rules precisely:\n",
    "1.  Identify the emotions expressed in the story.\n",
    "2.  Note any mentions of material conditions (e.g., shortages of PPE, crowded spaces).\n",
    "3.  Identify themes of solidarity or isolation.\n",
    "4.  Do not add any additional text or explanation outside of the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "story = \"\"\"\n",
    "We ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scaredâ€”though I wasâ€”but because I felt like no one saw us.\n",
    "\"\"\"\n",
    "\n",
    "# The system message provides the instructions and codebook\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": research_prompt},\n",
    "    {\"role\": \"user\", \"content\": story}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# The model's response will be a clean JSON object, ready for your analysis\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe41416-4827-4d34-8b31-69ef9be26bb3",
   "metadata": {},
   "source": [
    "Note: In this example, we've also introduced an optional parameter, `response_format={\"type\": \"json_object\"}`. This is a powerful feature that instructs the model to only return valid JSON, which is essential for a repeatable, programmatic workflow.\n",
    "\n",
    "This demonstrates how a strong system prompt can transform a general-purpose model into a specialized research assistant, ensuring that every request returns a consistent, structured output that you can easily process and analyze at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc10439-8c19-402e-952f-7117361401a5",
   "metadata": {},
   "source": [
    "### [Challenge]: Let's build a Multi-Turn Conversation Loop\n",
    "Now that you understand how to use the messages list to provide context, we can build a dynamic conversation. Instead of manually creating a new messages list for each turn, we will create a simple loop that appends the user's new message and the model's new response to the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625b5c9-3839-4e3a-9dc6-7e8fd17f0c77",
   "metadata": {},
   "source": [
    "#### ðŸ¥Š **Challenge 1**: Continue the Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4ef12c8-1836-4101-afa2-2466067e3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completions_response(message_history, new_user_message):\n",
    "\n",
    "    messages = message_history\n",
    "\n",
    "    \"\"\"\n",
    "        Hint: \n",
    "        We need to append our new user message to messages. Think about the most important parameters that go into a message request. \n",
    "    \"\"\"\n",
    "    messages.append(\n",
    "        # {\n",
    "        #     #... Fill this in, what 2 parameters are essential in a message request body?\n",
    "        #     \"role\": \"<REPLACE>\",\n",
    "        #     \"...\": \"<REPLACE>\"\n",
    "        # }\n",
    "\n",
    "        {\"role\": \"user\", \"content\": new_user_message}\n",
    "    )\n",
    "\n",
    "    model = \"deepseek/deepseek-chat-v3-0324:free\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d4cc1a0-e77d-4a1a-8c01-d10063e7a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1755483217-AJ1sB7l5UqlIf5fXHl9n', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! Here's a summary of the sentiment classification for the tweets:  \\n\\n- **In Favor (Positive)**: 2 tweets (pollution reduction, traffic improvement)  \\n- **Against (Negative)**: 2 tweets (seen as a money grab, burdens drivers)  \\n- **Neutral (Undecided)**: 1 tweet (needs more information)  \\n\\n### Breakdown:  \\n- The positive tweets highlight environmental and traffic benefits.  \\n- The negative tweets focus on financial strain and poor public transport conditions.  \\n- Only one user remains neutral, expressing uncertainty.  \\n\\nWould you like any further analysis?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1755483217, model='deepseek/deepseek-chat-v3-0324:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=126, prompt_tokens=387, total_tokens=513, completion_tokens_details=None, prompt_tokens_details=None), provider='Chutes')\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify the following tweets about the new city congestion tax into one of three categories: In Favor, Against, Neutral.\"},\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"\"\"\n",
    "            Tweets to classify:\n",
    "                \"Finally! The congestion tax will reduce traffic and make downtown air cleaner. About time the city acted!\"\n",
    "                \"This tax is just another way for politicians to squeeze money out of regular people. Terrible idea.\"\n",
    "                \"Not sure yet how the congestion tax will affect my commute. Need more info before I decide.\"\n",
    "                \"Public transport is already overcrowded, and now they want to charge us more to drive. Ridiculous.\"\n",
    "                \"Great move! Other cities did this and saw big drops in pollution.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Hereâ€™s the classification of the tweets:  \\n\\n1. **\"Finally! The congestion tax will reduce traffic and make downtown air cleaner. About time the city acted!\"**  \\n   â†’ **In Favor** (Positive sentiment, supports the policy)  \\n\\n2. **\"This tax is just another way for politicians to squeeze money out of regular people. Terrible idea.\"**  \\n   â†’ **Against** (Negative sentiment, criticizes the policy)  \\n\\n3. **\"Not sure yet how the congestion tax will affect my commute. Need more info before I decide.\"**  \\n   â†’ **Neutral** (Uncertainty, neither support nor opposition)  \\n\\n4. **\"Public transport is already overcrowded, and now they want to charge us more to drive. Ridiculous.\"**  \\n   â†’ **Against** (Negative sentiment, opposes the tax)  \\n\\n5. **\"Great move! Other cities did this and saw big drops in pollution.\"**  \\n   â†’ **In Favor** (Positive sentiment, highlights benefits of the policy)  \\n\\nLet me know if you\\'d like further refinements!\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response, messages = get_completions_response(messages, \"Can you summarize the results for me?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd3e45ee-9be1-4f42-986d-26ba119e5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_message_history_with_response(message_history, completion_response):\n",
    "    # Let's update our running message history with the response we just got back from the response.\n",
    "    # Stop here and ask yourself... why are we doing this?\n",
    "\n",
    "    # Hint: Look at the ChatCompletion Object. We just need to extract a specific object from this element\n",
    "    # There's two ways to do this, manually creating a JSON object or using the object directly from the ChatCompletion Object itself. \n",
    "    message_history.append(completion_response.choices[0].message)\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfa05626-e572-440a-9658-1711f132c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
      " {'content': 'Classify the following tweets about the new city congestion tax '\n",
      "             'into one of three categories: In Favor, Against, Neutral.',\n",
      "  'role': 'user'},\n",
      " {'content': '\\n'\n",
      "             '            Tweets to classify:\\n'\n",
      "             '                \"Finally! The congestion tax will reduce traffic '\n",
      "             'and make downtown air cleaner. About time the city acted!\"\\n'\n",
      "             '                \"This tax is just another way for politicians to '\n",
      "             'squeeze money out of regular people. Terrible idea.\"\\n'\n",
      "             '                \"Not sure yet how the congestion tax will affect '\n",
      "             'my commute. Need more info before I decide.\"\\n'\n",
      "             '                \"Public transport is already overcrowded, and '\n",
      "             'now they want to charge us more to drive. Ridiculous.\"\\n'\n",
      "             '                \"Great move! Other cities did this and saw big '\n",
      "             'drops in pollution.\\n'\n",
      "             '        ',\n",
      "  'role': 'user'},\n",
      " {'content': 'Hereâ€™s the classification of the tweets:  \\n'\n",
      "             '\\n'\n",
      "             '1. **\"Finally! The congestion tax will reduce traffic and make '\n",
      "             'downtown air cleaner. About time the city acted!\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, supports the policy)  \\n'\n",
      "             '\\n'\n",
      "             '2. **\"This tax is just another way for politicians to squeeze '\n",
      "             'money out of regular people. Terrible idea.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, criticizes the policy)  \\n'\n",
      "             '\\n'\n",
      "             '3. **\"Not sure yet how the congestion tax will affect my '\n",
      "             'commute. Need more info before I decide.\"**  \\n'\n",
      "             '   â†’ **Neutral** (Uncertainty, neither support nor '\n",
      "             'opposition)  \\n'\n",
      "             '\\n'\n",
      "             '4. **\"Public transport is already overcrowded, and now they want '\n",
      "             'to charge us more to drive. Ridiculous.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, opposes the tax)  \\n'\n",
      "             '\\n'\n",
      "             '5. **\"Great move! Other cities did this and saw big drops in '\n",
      "             'pollution.\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, highlights benefits of '\n",
      "             'the policy)  \\n'\n",
      "             '\\n'\n",
      "             \"Let me know if you'd like further refinements!\",\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Can you summarize the results for me?', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9151045-e1a8-41b5-a203-de63b4b43a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
      " {'content': 'Classify the following tweets about the new city congestion tax '\n",
      "             'into one of three categories: In Favor, Against, Neutral.',\n",
      "  'role': 'user'},\n",
      " {'content': '\\n'\n",
      "             '            Tweets to classify:\\n'\n",
      "             '                \"Finally! The congestion tax will reduce traffic '\n",
      "             'and make downtown air cleaner. About time the city acted!\"\\n'\n",
      "             '                \"This tax is just another way for politicians to '\n",
      "             'squeeze money out of regular people. Terrible idea.\"\\n'\n",
      "             '                \"Not sure yet how the congestion tax will affect '\n",
      "             'my commute. Need more info before I decide.\"\\n'\n",
      "             '                \"Public transport is already overcrowded, and '\n",
      "             'now they want to charge us more to drive. Ridiculous.\"\\n'\n",
      "             '                \"Great move! Other cities did this and saw big '\n",
      "             'drops in pollution.\\n'\n",
      "             '        ',\n",
      "  'role': 'user'},\n",
      " {'content': 'Hereâ€™s the classification of the tweets:  \\n'\n",
      "             '\\n'\n",
      "             '1. **\"Finally! The congestion tax will reduce traffic and make '\n",
      "             'downtown air cleaner. About time the city acted!\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, supports the policy)  \\n'\n",
      "             '\\n'\n",
      "             '2. **\"This tax is just another way for politicians to squeeze '\n",
      "             'money out of regular people. Terrible idea.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, criticizes the policy)  \\n'\n",
      "             '\\n'\n",
      "             '3. **\"Not sure yet how the congestion tax will affect my '\n",
      "             'commute. Need more info before I decide.\"**  \\n'\n",
      "             '   â†’ **Neutral** (Uncertainty, neither support nor '\n",
      "             'opposition)  \\n'\n",
      "             '\\n'\n",
      "             '4. **\"Public transport is already overcrowded, and now they want '\n",
      "             'to charge us more to drive. Ridiculous.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, opposes the tax)  \\n'\n",
      "             '\\n'\n",
      "             '5. **\"Great move! Other cities did this and saw big drops in '\n",
      "             'pollution.\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, highlights benefits of '\n",
      "             'the policy)  \\n'\n",
      "             '\\n'\n",
      "             \"Let me know if you'd like further refinements!\",\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Can you summarize the results for me?', 'role': 'user'},\n",
      " ChatCompletion(id='gen-1755476231-YSA7WDHUbBfMotukN21A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1755476231, model='deepseek/deepseek-chat-v3-0324:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=163, prompt_tokens=387, total_tokens=550, completion_tokens_details=None, prompt_tokens_details=None), provider='Targon'),\n",
      " ChatCompletion(id='gen-1755476231-YSA7WDHUbBfMotukN21A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1755476231, model='deepseek/deepseek-chat-v3-0324:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=163, prompt_tokens=387, total_tokens=550, completion_tokens_details=None, prompt_tokens_details=None), provider='Targon'),\n",
      " ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)]\n"
     ]
    }
   ],
   "source": [
    "messages = update_message_history_with_response(messages, response)\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc740d-b683-4f6c-beaf-26328092d87b",
   "metadata": {},
   "source": [
    "#### Challenge 2: Put this together to build a chat conversation tool like ChatGPT that will remember what you say in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f36a0f9-dd65-4d3e-96eb-2c913c085a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask anything... (or 'quit' to exit):  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask anything... (or 'quit' to exit):  what did i just say?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You just said: *\"hi\"*  \n",
      "\n",
      "And before that, you asked: *\"what did i just say?\"*  \n",
      "\n",
      "Iâ€™m happy to help with anything you needâ€”just let me know! ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask anything... (or 'quit' to exit):  quit\n"
     ]
    }
   ],
   "source": [
    "message_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "while True:\n",
    "    user_input = input(\"Ask anything... (or 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    response, messages = get_completions_response(message_history, user_input)\n",
    "    print(response.choices[0].message.content)\n",
    "    message_history = messages\n",
    "    update_message_history_with_response(message_history, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413e396-bc71-4361-86b9-c3becbe80348",
   "metadata": {},
   "source": [
    "# Zero-Shot vs. Few-Shot Prompting\n",
    "\n",
    "Now that you have a firm grasp of the API's mechanics, let's explore how to get the model to produce the exact output you need. This is where the practice of prompting comes in.\n",
    "\n",
    "We'll return to our COVID-19 scenario and the task of thematic coding. We want the model to act as a human coder, identifying specific themes from the essential workers' narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b5e73-0879-484f-aa90-8c0b680fed49",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting: \"Just Ask For It\"\n",
    "\n",
    "Zero-shot prompting is when you ask the model to perform a task without giving it any examples. You rely entirely on the model's pre-trained knowledge to understand your request.\n",
    "\n",
    "Let's try this with one of our narratives. We will give the model the story and simply ask it to extract the themes, without any examples or explicit instructions on the output format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60750fd-fbe6-4fb9-856d-637ee54c1c9e",
   "metadata": {},
   "source": [
    "ðŸ’¡ Tip: In most cases, you don't have to think too much about the system prompt. Keeping it at a short and simple `{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}` will suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64667d94-7715-47de-bcd5-e0787b43d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new narrative from an essential worker\n",
    "narrative = \"\"\"\n",
    "People think of frontâ€‘line workersâ€¦the grocery workers, transit workers, the first respondersâ€¦ as having helped the city get through it. But thatâ€™s not what happened. We helped the city survive it.\n",
    "\"\"\"\n",
    "\n",
    "# The messages list for our Zero-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Analyze the following story and extract key themes: {narrative}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a4cfd87-a6ba-46c3-ae70-bc91ace9f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story you've shared is a poignant reflection on the role of front-line workers during a crisis, likely referencing the COVID-19 pandemic or a similar societal challenge. Here are the key themes extracted from the passage:\n",
      "\n",
      "1. **Essential Labor and Survival**: The narrative highlights the critical role of front-line workers (grocery workers, transit workers, first responders) in ensuring the literal survival of the city during a crisis. It shifts the focus from mere \"getting through\" a hardship to emphasizing their role in sustaining life and basic societal functions.\n",
      "\n",
      "2. **Underappreciation and Misperception**: The statement challenges a common perceptionâ€”that these workers merely \"helped the city get through it\"â€”by asserting a deeper truth: their labor was foundational to survival. This suggests a frustration with how their contributions might be minimized or oversimplified by society.\n",
      "\n",
      "3. **Sacrifice and Resilience**: The tone implies that front-line workers bore a disproportionate burden, enduring risks (e.g., health hazards, long hours) to keep others safe and the city functioning. Their actions are framed as an act of collective resilience.\n",
      "\n",
      "4. **Intersection of Visibility and Value**: Grocery and transit workers, often undervalued in pre-crisis times, are elevated to the same level as first responders, underscoring how crises reveal the true indispensability of certain roles.\n",
      "\n",
      "5. **Collective Dependence on Marginalized Labor**: The story subtly critiques societal structures that rely on often underpaid or overlooked workers in times of need, questioning whether this dependence is met with fair recognition or systemic change.\n",
      "\n",
      "6. **Semantic Resistance**: The shift from \"get through\" to \"survive\" is a deliberate linguistic choice, reclaiming the narrative to reflect the gravity of the workers' contributions. It challenges listeners to reframe their understanding of crisis and labor.\n",
      "\n",
      "**Nuanced Takeaway**: The story is both a tribute to front-line workers and a critique of how society acknowledges (or fails to acknowledge) their existential role. It calls for a deeper recognition of labor that is deemed \"essential\" only in hindsight.\n"
     ]
    }
   ],
   "source": [
    "# Send the request\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aece1d8-0dde-49bb-bd56-c0ea52e5b447",
   "metadata": {},
   "source": [
    "---\n",
    "The model will likely respond with a good analysis, but the format will be inconsistent. It might be a list, a paragraph, or a different style each time, which makes it very difficult to process programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5ed38-5880-4394-934b-70a7d8b2356a",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting: Let's give it some examples to show it what we want\n",
    "\n",
    "Few-shot prompting is when you give the model one or more examples of the input and the desired output. By showing it what you want, you significantly increase the likelihood of getting a consistent, structured response.\n",
    "\n",
    "This approach is like giving a new researcher a codebook with a few pre-coded examples to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcffced-63cf-45ef-ae3b-2fe367ca653c",
   "metadata": {},
   "source": [
    "At the simplest level, you can think of few-shot prompting as giving pointers or guidance to the LLM on how to respond. It's similar to how you might teach someone a new task by showing them a few clear examples rather than just describing the process. This approach is invaluable for getting consistent, structured output from the model.\n",
    "\n",
    "Let's see this in action with a few simple scenarios.\n",
    "\n",
    "### Scenario 1: Thematic Identification\n",
    "\n",
    "Imagine you want to identify the main theme of a short story. A simple instruction (zero-shot) might not give you the format you need.\n",
    "\n",
    "* **Zero-Shot Prompt:** \"What is the theme of this story about a detective who solves a case using a seemingly insignificant detail?\"\n",
    "* **Likely Zero-Shot Response:** \"The theme of the story is attention to detail, as the detective's success hinges on a small, overlooked fact.\"\n",
    "This is a good response, but what if you wanted it to be just a single word? Let's use few-shot prompting to guide it.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"A young woman moves to a new city and learns to navigate a demanding job and a new social circle.\"\n",
    "    * **Output 1:** \"Theme: Coming-of-age\"\n",
    "    * **Input 2:** \"Two rival knights must team up to defeat a dragon that is terrorizing their kingdoms.\"\n",
    "    * **Output 2:** \"Theme: Collaboration\"\n",
    "    * **Input 3:** \"A story about a detective who solves a case using a seemingly insignificant detail.\"\n",
    "* **Likely Few-Shot Response:** \"Theme: Attention to detail\"\n",
    "By giving just two examples, we've taught the model the exact output format we want: Theme: [single word].\n",
    "\n",
    "### Scenario 2: Simple Data Extraction\n",
    "\n",
    "What if you need to extract specific information, like names and dates?\n",
    "\n",
    "* **Zero-Shot Prompt:** \"Find the person's name and birthday in this sentence: 'The grand opening, attended by CEO Jane Doe, was held on October 26, 2024, to celebrate her 45th birthday.'\"\n",
    "* **Likely Zero-Shot Response:** \"The person's name is Jane Doe and her 45th birthday is on October 26, 2024.\"\n",
    "Again, the response is correct, but not in a format you can easily use in a spreadsheet or database. Now let's try a few-shot prompt to get a clean, structured list.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"The presentation was given by Dr. Alan Turing on June 23, 1912.\"\n",
    "    * **Output 1:** \"Name: Alan Turing, Date: June 23, 1912\"\n",
    "    * **Input 2:** \"Marie Curie's discovery on November 7, 1867, changed the world.\"\n",
    "    * **Output 2:** \"Name: Marie Curie, Date: November 7, 1867\"\n",
    "    * **Input 3:** \"The grand opening, attended by CEO Jane Doe, was held on October 26, 2024, to celebrate her 45th birthday.\"\n",
    "* **Likely Few-Shot Response:** \"Name: Jane Doe, Date: October 26, 2024\"\n",
    "\n",
    "### Scenario 3: Abstract Text Analysis\n",
    "\n",
    "Few-shot prompting can even be used for more abstract tasks, like determining a character's alignment based on their actions, a common task in fantasy or game-related analysis.\n",
    "\n",
    "* **Zero-Shot Prompt:** \"Is a character who regularly disobeys laws to help others and who values personal freedom over societal order good, evil, or neutral?\"\n",
    "* **Likely Zero-Shot Response:** \"A character who regularly disobeys laws to help others and values personal freedom over societal order could be considered a chaotic good character in many alignment systems.\"\n",
    "This is a good, detailed answer, but what if you want a simpler classification? We can provide examples of what \"lawful,\" \"chaotic,\" \"good,\" and \"evil\" mean in practice to guide the model's output.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"A character who robs a corrupt merchant to give gold to the poor, believing personal conscience is more important than law.\"\n",
    "    * **Output 1:** \"Alignment: Chaotic Good\"\n",
    "    * **Input 2:** \"A character who follows every law and rule to the letter, even if it leads to a bad outcome.\"\n",
    "    * **Output 2:** \"Alignment: Lawful Neutral\"\n",
    "    * **Input 3:** \"A character who regularly disobeys laws to help others and who values personal freedom over societal order.\"\n",
    "* **Likely Few-Shot Response:** \"Alignment: Chaotic Good\"\n",
    "As you can see, the model learns the exact structure from the examples and applies it to the new input, making the output predictable and machine-readable. This simple technique is the foundation for getting the clean JSON responses we need for our research workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6a9ce-66d1-4c3d-8e7e-49fa95de62b1",
   "metadata": {},
   "source": [
    "### Few Shot Examples with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a7530-5035-465a-9a83-0fca2d56f737",
   "metadata": {},
   "source": [
    "Let's go back to our coding task. We want the output to be a clean JSON object with specific keys. We can provide the model with an example of a coded narrative to guide its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2611e2b-18db-44a9-9071-28ed91335549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"emotion\": [\"fear\", \"sadness\", \"frustration\"],\n",
      "  \"material_conditions\": [\"lack of resources\"],\n",
      "  \"solidarity\": \"absent\",\n",
      "  \"theme\": \"invisibility of labor\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# A new narrative for the model to analyze\n",
    "narrative_2 = \"\"\"\n",
    "We ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scaredâ€”though I wasâ€”but because I felt like no one saw us.\n",
    "\"\"\"\n",
    "\n",
    "# The message containing the example\n",
    "example_prompt = \"\"\"\n",
    "Here is an example of a coded narrative and its desired JSON output:\n",
    "\n",
    "**Narrative:**\n",
    "People think of frontâ€‘line workersâ€¦the grocery workers, transit workers, the first respondersâ€¦ as having helped the city get through it. But thatâ€™s not what happened. We helped the city survive it.\n",
    "\n",
    "**Coded JSON Output:**\n",
    "{\n",
    "  \"emotion\": [\"anger\", \"resilience\"],\n",
    "  \"material_conditions\": [\"none\"],\n",
    "  \"solidarity\": \"absent\",\n",
    "  \"theme\": \"invisibility of labor\"\n",
    "}\n",
    "\n",
    "Now, please code the following narrative using the same format.\n",
    "\"\"\"\n",
    "\n",
    "# The messages list for our Few-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert qualitative researcher.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{example_prompt}\\n\\n**Narrative to code:**\\n{narrative_2}\"}\n",
    "]\n",
    "\n",
    "# Send the request with the example\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "# The model is now much more likely to respond with a valid JSON object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880f94f-d159-497c-ac29-52463b71a1c7",
   "metadata": {},
   "source": [
    "---\n",
    "Key Takeaways\n",
    "- Zero-shot is great for simple, general tasks, but it lacks control over the output format.\n",
    "- Few-shot is your best friend when you need the model to follow a specific format or style. The examples you provide are crucial for guiding the model's response and ensuring consistency, which is vital for programmatic analysis.\n",
    "\n",
    "By combining few-shot prompting with a powerful system prompt and the response_format parameter, you can build a highly reliable and scalable data extraction tool for your research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2cacb1-bbb0-4912-8782-3dc6f2485f1f",
   "metadata": {},
   "source": [
    "Providing examples seemed to help the model return data in a format, more closely related to what we want. But notice, it still adds surrounding context and follow up questions. \n",
    "\n",
    "ðŸ”” Question: Why would this response still be suboptimal if you were a researcher trying to extract information at scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42574114-8d5c-4354-a7e9-ec8420a07624",
   "metadata": {},
   "source": [
    "## Structured Output: Guaranteed Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ebfe5-04bb-4ad1-8685-258b2f362ba0",
   "metadata": {},
   "source": [
    "Structred output refers to the ability of the `ChatCompletions` API to return responses in a predefined format, such as a JSON object or a Pydantic Model. This is particulay useful when you need the model to adhere to a specific schema for downstram processing or integration with other systems. By defining the expected structure, you can ensure the response is validated and parsed into a predictable format. \n",
    "\n",
    "Key Features of Structured Outputs\n",
    "\n",
    "1. Customizable Response Format\n",
    "    - You can specify the expected structure of the response using the response_format parameter.\n",
    "    - This can be defined as either a JSON schema or a Pydantic model, depending on your requirements.\n",
    "2. Using JSON Schema with create:\n",
    "    - The `chat.completions.create` method allows you to provide a JSON schema via the `response_format` paramater.\n",
    "    - This guides the model to generate responses in the desired structure without requiring Python-based schema definitions.\n",
    "3. Using Pydantic Models with parse\n",
    "    - The `chat.completions.parse` method supports validation and parsing using Pydantic models.\n",
    "    - This is ideal for scenarios where you need Python-based schema definitions and strict adherance to the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba34422-2c98-449a-b53e-f85136cc195c",
   "metadata": {},
   "source": [
    "### Setting up Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ae52e9c-8ea7-4356-a7c3-9c2e0e2a55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the expected structure of the response\n",
    "class ParsedSentence(BaseModel):\n",
    "    subject: str\n",
    "    verb: str\n",
    "    obj: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c57a1171-2c16-40be-bf7a-1fc3497c7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request to extract parts of a simple sentence\n",
    "response = client.chat.completions.parse(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Extract the grammatical components from the sentence.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"The cat chased the mouse.\"},\n",
    "    ],\n",
    "    response_format=ParsedSentence,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ac3a6f1-c942-4f0f-8dab-df70a2a01a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParsedChatCompletionMessage[ParsedSentence](content='{  \\n  \"subject\": \"The cat\",  \\n  \"verb\": \"chased\",  \\n  \"obj\": \"the mouse\"  \\n}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=ParsedSentence(subject='The cat', verb='chased', obj='the mouse'), reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db289817-67f9-4f67-97c8-b912745274eb",
   "metadata": {},
   "source": [
    "ðŸ”” Question: How can we extract our parsed message from this `ParsedChatCompletionMessage` Object? What fields can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4709e5ec-a6c6-4b5e-b403-c4fa9d944318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject='The cat' verb='chased' obj='the mouse'\n",
      "Subject: The cat\n",
      "Verb: chased\n",
      "Obj: the mouse\n"
     ]
    }
   ],
   "source": [
    "# To extract our Structured Response\n",
    "print(response.choices[0].message.parsed)\n",
    "\n",
    "print(\"Subject:\", response.choices[0].message.parsed.subject)\n",
    "print(\"Verb:\", response.choices[0].message.parsed.verb)\n",
    "print(\"Obj:\", response.choices[0].message.parsed.obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bb5b6-bc63-48ec-b121-f9229a27909e",
   "metadata": {},
   "source": [
    "### ðŸ¥Š [Final Challenge]: Putting it all togther - Classification using Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0994690-439e-4944-92c8-80932657181d",
   "metadata": {},
   "source": [
    "The goal of this final challenge is to combine everything you've learned to build a reliable, end-to-end workflow for structured data extraction. You will take a new narrative from an essential worker and use a Pydantic model to extract key themes in a validated, structured format.\n",
    "\n",
    "Your Goal:\n",
    "Using the DeepSeek API, your task is to create a Python script that takes the provided new_narrative as input and produces the expected_output by:\n",
    "1. **Defining a Pydantic Model**: Create a BaseModel that accurately represents the structure of the expected_output.\n",
    "2. **Building the Prompt**: Construct a messages list that includes the system prompt, a few-shot example, and the final user message containing the new_narrative.\n",
    "3. **Making the API Call**: Use the client.chat.completions.create method to call the DeepSeek model.\n",
    "4. **Validating the Response**: Use your Pydantic model to validate and parse the raw JSON string returned by the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64874045-2985-43a5-b006-9892ddd996dc",
   "metadata": {},
   "source": [
    "#### The Input\n",
    "\n",
    "This is the narrative you will be using as input for the LLM.\n",
    "\n",
    "```python\n",
    "new_narrative = \"\"\"\n",
    "In the quiet of the night, I'd mop floors at the hospital, the only sound the soft swish of the bucket. Patients came and went, doctors hurried past. Sometimes, they'd look right through me. But I'd always tell myself: someone has to keep this place clean for the healers to do their healing. That thought got me through the loneliest shifts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77d20e-f953-4e1b-854b-1f883188e7cc",
   "metadata": {},
   "source": [
    "#### The Expected Output\n",
    "Your script should produce a Pydantic object that, when printed, looks like this. This is your target.\n",
    "\n",
    "```python\n",
    "ThematicAnalysis(emotion=['loneliness', 'dedication'], material_conditions=['hospital cleaning'], solidarity='present', theme='invisibility of labor and pride')```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b62d05-d02b-4554-94d9-d8a1e53803dc",
   "metadata": {},
   "source": [
    "Hint: You will need to define the ThematicAnalysis Pydantic class to have fields for emotion, material_conditions, solidarity, and theme, just like the previous examples. Be sure to use the correct data types and a Literal type for the solidarity field. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4b1c5-46d6-49f3-89d1-f6704a80f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "# (Assuming client is already initialized)\n",
    "client = OpenAI(\n",
    "    api_key=\"...\",  # Replace with your API key\n",
    "    base_url=\"[https://api.deepseek.com/v1](https://api.deepseek.com/v1)\"\n",
    ")\n",
    "\n",
    "# The new narrative to analyze\n",
    "new_narrative = \"\"\"\n",
    "In the quiet of the night, I'd mop floors at the hospital, the only sound the soft swish of the bucket. Patients came and went, doctors hurried past. Sometimes, they'd look right through me. But I'd always tell myself: someone has to keep this place clean for the healers to do their healing. That thought got me through the loneliest shifts.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Define the Pydantic class here to match the desired output structure.\n",
    "# Hint: Your class should have fields for 'emotion', 'material_conditions', 'solidarity', and 'theme'.\n",
    "#       Remember to use the correct data types!\n",
    "#\n",
    "#       You may need to look up how to allow list datatypes in Pydantic. \n",
    "# class ThematicAnalysis(...):\n",
    "#     ...\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Build the few-shot prompt.\n",
    "# Hint: The messages list needs a system prompt, a user/assistant example pair,\n",
    "#       and the final user message with the new narrative.\n",
    "#\n",
    "# messages = [\n",
    "#     ...\n",
    "# ]\n",
    "\n",
    "\n",
    "# Step 3: Make the API call to the DeepSeek model using `.parse`.\n",
    "# Hint: The `.parse` method handles the validation and returns a Pydantic object directly.\n",
    "#\n",
    "# parsed_analysis = client.chat.completions.parse(\n",
    "#     model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "#     messages=messages,\n",
    "#     response_format=...  # Specify your Pydantic class here\n",
    "# )\n",
    "\n",
    "\n",
    "# # The `parsed_analysis` variable now holds a validated Pydantic object!\n",
    "# print(parsed_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ae4a3-693e-43f2-ad28-09201f8a427d",
   "metadata": {},
   "source": [
    "## Conclusion: What We've Learned\n",
    "\n",
    "In this workshop, you've moved from the conceptual understanding of LLM APIs to hands-on, programmatic control. You now have the foundational knowledge to build powerful, automated workflows with language models.\n",
    "\n",
    "Hereâ€™s a quick summary of the key concepts youâ€™ve mastered:\n",
    "\n",
    "1.  **API Mechanics:** You understand that LLM APIs are **stateless**. To simulate memory and maintain context, you are **responsible for managing the message history** yourself by sending a full list of messages with each new request.\n",
    "2.  **The Power of Roles:** You know how to use the `system`, `user`, and `assistant` roles to give the model instructions, provide it with your prompts, and capture its responses. The `system` role is particularly powerful for setting high-level rules and persona.\n",
    "3.  **Zero-Shot vs. Few-Shot Prompting:** You've seen how **zero-shot** prompting is great for general tasks but can result in inconsistent output. In contrast, **few-shot** prompting is essential for guiding the model to produce a consistent, predictable format by providing it with a single example or a few examples.\n",
    "4.  **Structured Output:** You now have the ultimate tool for reliability: **structured output with Pydantic**. By defining a `BaseModel`, you can give the LLM a clear blueprint for its response, and the `.parse` method ensures the output is always a valid, usable Python object. This is the critical step for moving from simple chat to scalable data analysis.\n",
    "\n",
    "The specific example we worked through of thematic coding of a narrative is just one use case. The most powerful concept youâ€™ve learned today is the idea of using structured output to programmatically define how the LLM should respond. This is a foundational technique that you can apply across all social science workflows, whether you are doing **data extraction**, **summarization**, or **classification**. By providing the LLM with a schema, you gain precise control over its output, making it a reliable and powerful tool for your research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
