{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56350131-6ebf-4309-a656-7bc472013c3f",
   "metadata": {},
   "source": [
    "# Python APIs for Large Language Models\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives\n",
    "    \n",
    "* Differentiate between large language models (LLMs) and the applications built on top of them (e.g., ChatGPT, Claude)\n",
    "* Understand how to programmatically interface with LLMs using APIs\n",
    "* Apply LLMs to social science research for tasks such as thematic coding, content classification, and structured data extraction at scale\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üìù **Poll:** A Zoom poll to help you learn!<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br> \n",
    "\n",
    "### Sections\n",
    "1. [This Workshop](#)\n",
    "2. [LLMs as a Research Tool](#)\n",
    "3. [Motivating Scenario: Thematic Coding](#)\n",
    "    4. [Approach 1: Manual Coding](#)\n",
    "    5. [Approach 2: ChatGPT](#)\n",
    "    6. [Approach 3: The API](#)\n",
    "4. [LLM Hosting and Open Router](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf513e-04ba-4025-98a3-870385f3f3a1",
   "metadata": {},
   "source": [
    "<a id='this'></a>\n",
    "\n",
    "## This Workshop\n",
    "\n",
    "Most of us have used tools like ChatGPT or Claude, but what you may not realize is that these interfaces are **not the language models themselves**. They‚Äôre polished applications built on top of large language models (LLMs), adding helpful features like memory, chat formatting, context injection, and more.\n",
    "\n",
    "In this workshop, we‚Äôll peel back the layers and explore:\n",
    "- **How LLMs are hosted and accessed through APIs**\n",
    "- **How tools like ChatGPT are built around them**\n",
    "- **How LLM's can be useful in social science research**\n",
    "- **How you can interface directly with LLMs via API calls to power your social science research workflows**\n",
    "\n",
    "This shift from using LLMs in chat to using them as infrastructure unlocks powerful possibilities for research:\n",
    "- Extracting structured data from unstructured text  \n",
    "- Performing thematic coding across interview transcripts  \n",
    "- Generating summaries, classifications, and annotations at scale  \n",
    "\n",
    "By the end, you'll know how to go from ‚Äúlet me ask ChatGPT a question‚Äù to let me integrate LLMs into my own social science research workflow to classify data, extract insights, and analyze content at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ace53a-8f81-4730-bf3a-847ef0eb7911",
   "metadata": {},
   "source": [
    "## LLMs as a Research Tool\n",
    "\n",
    "Before we even get into the programmatic use of LLMs, let's break down a few of the methods and how they can be useful in your social science workflow. \n",
    "\n",
    "Across a wide variety of research workflows, LLMs can be used to: \n",
    "- **Analyze Qualitative Data**: Perform thematic coding, sentiment analysis, and information extraction from interview transcripts, field notes, and open-ended survey responses.\n",
    "    - *For example*: A social scientist could automatically identify recurring themes like \"systemic barriers\" or \"lack of institutional support\" across 50 interview transcripts, saving dozens of hours of manual review.\n",
    "- **Classify Content**: Categorize thousands of social media posts, news articles, or legal documents based on predefined criteria.\n",
    "    - *For example*: A political scientist could classify thousands of tweets about a new policy as \"in favor,\" \"against,\" or \"neutral,\" allowing for large-scale analysis of public opinion.\n",
    "- **Summarize and Synthesize**: Generate concise summaries of long texts, helping you quickly identify key arguments and recurring patterns across a large corpus.\n",
    "    - *For example*: A historian could feed a model hundreds of personal letters from a historical period and ask it to identify common sentiments or events, providing a high-level overview before a deeper dive.\n",
    "- **Create Structured Data with `StructuredOutput`**: Transform messy, unstructured text into clean, structured data (e.g., JSON) that is ready for quantitative analysis.\n",
    "    - *For example*: An urban planner could extract specific details like a building's address, construction date, and historical zoning designation from a collection of digitized city reports, building a database for further analysis. This workshop will focus on the last two points, showing you how to go from unstructured text to usable data at scale. The following scenario is just one example of how this can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387d53e-a331-4af2-8cd0-52189e17c535",
   "metadata": {},
   "source": [
    "## Motivating Scenario: Thematic Coding of COVID-19 Narratives\n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "üí° <i>Tip</i>: In this workshop, we will anchor our learning to one specific case study, demonstrating how to transform unstructured text into usable data at scale. However, the techniques you will learn here are universally applicable to a wide range of research challenges.\n",
    "</div>\n",
    "\n",
    "Imagine you're a social science researcher helping compile a book like [When the City Stopped](https://www.cornellpress.cornell.edu/book/9781501780387/when-the-city-stopped/), a project that gathered, sorted, and categorized stories from essential workers across New York City during the COVID-19 pandemic. You‚Äôve sent out an open call and collected hundreds of open-ended stories from nurses, EMTs, grocery clerks, MTA workers, and others. These personal narratives capture moments of fear, resilience, trauma, grief, and hope. You want to analyze them systematically.\n",
    "\n",
    "You get a few responses back like:\n",
    "- > \"People think of front‚Äëline workers‚Ä¶the grocery workers, transit workers, the first responders‚Ä¶ as having helped the city get through it. But that‚Äôs not what happened. We helped the city survive it.\"\n",
    "- > \"We ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scared - though I was - but because I felt like no one saw us.\"\n",
    "\n",
    "Your goal is to identify:\n",
    "- The emotions expressed in each story\n",
    "- Mentions of material conditions (e.g. PPE shortages)\n",
    "- Instances of collective solidarity or isolation\n",
    "- Themes of grief, duty, or burnout\n",
    "\n",
    "And then categorize, sort, and cluster these examples based on the content and themes present. \n",
    "\n",
    "---\n",
    "\n",
    "### Approach 1: Manual Thematic Coding (Example)\n",
    "\n",
    "![Manual Thematic Coding](../images/manual_approach.png)\n",
    "In traditional qualitative research, this would involve:\n",
    "- Reading a sample of stories manually\n",
    "- Developing a codebook of recurring themes\n",
    "- Having multiple researchers apply those codes to each story\n",
    "- Negotiating disagreements, refining the codes, then repeating for 100s of entries\n",
    "\n",
    "Let‚Äôs say one narrative reads:\n",
    "> ‚ÄúWe ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scared‚Äîthough I was‚Äîbut because I felt like no one saw us.‚Äù\n",
    "\n",
    "A human-coded version might look like:\n",
    "- emotion: grief, exhaustion\n",
    "- material_conditions: PPE shortage\n",
    "- solidarity: absent\n",
    "- theme: invisibility of labor\n",
    "\n",
    "Though this process is rigorous; it is also slow, expensive, and hard to scale.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach 2: ChatGPT\n",
    "![Manual Thematic Coding](../images/chatGPT_approach.png)\n",
    "At some point, you realize: \"What if I just paste this into ChatGPT and ask it to extract that structured information for me!\" \n",
    "And you try it, and it works! Voila. Seems simple enough and you get a clean response like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"emotion\": [\"grief\", \"exhaustion\"],\n",
    "  \"material_conditions\": [\"PPE shortage\"],\n",
    "  \"solidarity\": \"absent\",\n",
    "  \"theme\": \"invisibility of labor\"\n",
    "}\n",
    "```\n",
    "\n",
    "But you still Have thousands of more more samples to get through. You still have to:\n",
    "- Manually copy and paste each sample into the UI\n",
    "- Copy and append the output to a document or spreadsheet\n",
    "- Hope ChatGPT stays consistent\n",
    "- And most importantly, repeat this process thousands of times\n",
    "\n",
    "Seems like something that would be made easy if you could just programmatically call ChatGPT or whichever the model is that it utilizes behind the scenes.\n",
    "\n",
    "    \n",
    "### Breakthrough Approach: Interfacing directly with the Large Language Models behind GPT\n",
    "![Manual Thematic Coding](../images/llm_automated.png)\n",
    "\n",
    "Instead of working through a web interface, what if you could:\n",
    "- Programmatically send each sample to the model  \n",
    "- Receive structured, reliable JSON responses  \n",
    "- Save everything automatically into a CSV or database  \n",
    "- Scale up from 10 samples to 10,000\n",
    "\n",
    "\n",
    "üí° **Tip**: We will enable this through a special tool called StructuredOuput which will be covered later in this workshop, but for now hold on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89610c-76c8-4a3f-bda8-eecf5967849b",
   "metadata": {},
   "source": [
    "## Background: LLM Hosting\n",
    "Now before we get into how you can interface with these LLM's directly through the API, let's build up some context to how LLM's are even hosted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac716ff-2121-45e4-8d93-92e141040b72",
   "metadata": {},
   "source": [
    "<img src=\"../images/chat_gpt_llm_server.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ddff-0161-45b7-9ace-9df825d204a6",
   "metadata": {},
   "source": [
    "### LLM Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9121e02-a383-4702-9f9f-1b2bb4efd462",
   "metadata": {},
   "source": [
    "At its core, a Large Language Model is just a giant neural network:  a trained set of weights and biases. When you ask it something you‚Äôre running what‚Äôs called a **forward pass or inference**. You pass in some input text, and the model predicts the next most likely word (again and again, until it's done). Theoretically, if you had a computer powerful enough you could run this set of weights and biases locally on your computer. \n",
    "\n",
    "However, in practice the SoTa (state of the art models) tend to be much, much larger than what you can fit on your computer. When you use ChatGPT, Claude, or Gemini, you‚Äôre not actually running the model on your laptop. Behind the scenes, you're sending a request to a Large Language Model (LLM) hosted on a powerful server that is capable of running these giant models. They can require hundreds of gigabytes of memory and need specialized hardware like GPUs.\n",
    "\n",
    "### Hosted LLMs and API Servers\n",
    "Because running an LLM requires this massive amount of compute resources, companies host them on the cloud and provide access through something called an API Server.\n",
    "\n",
    "An API (Application Programming Interface) server gives your application a way to:\n",
    "- Send text to the model (request),\n",
    "- Run an inference (get predictions) remotely,\n",
    "- And receive the result back (response)\n",
    "\n",
    "So when you interact with ChatGPT, what‚Äôs really happening under the hood is something like this:\n",
    "ChatGPT UI  ‚Üí  API Call  ‚Üí  Hosted LLM  ‚Üí  Output  ‚Üí  ChatGPT UI\n",
    "\n",
    "This pattern became the foundation for how LLMs are accessed at scale.\n",
    "\n",
    "### Competing Standards for LLM APIs\n",
    "As more models were released, different organizations built their own API ‚Äúspecifications‚Äù for hosting LLMs. A few major ones emerged:\n",
    "\n",
    "- OpenAI API Spec ‚Äì The most widely adopted standard, designed by OpenAI\n",
    "- Hugging Face TGI (Text Generation Inference) ‚Äì Hugging Face‚Äôs approach to model serving\n",
    "- vLLM ‚Äì A fast, GPU-efficient serving engine from UC Berkeley & partners\n",
    "\n",
    "Each of these defined:\n",
    "- What endpoints (like /generate or /chat) should exist\n",
    "- How to format the request body (prompts, parameters, etc.)\n",
    "- What the output should look like (tokens, probabilities, completions...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14893e-e1ac-48c3-98e5-4805bb8bc553",
   "metadata": {},
   "source": [
    "### The World Converges: OpenAI's API Format\n",
    "Similar to how phones began to converge on a single charging standard (like USB-C), the AI developer ecosystem has increasingly converged on OpenAI‚Äôs API format as a common interface for LLMs. Today, even third-party models like Mistral, Meta‚Äôs LLaMA, and others are often hosted behind APIs that support OpenAI-style endpoints, such as:\n",
    "\n",
    "- [Responses](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [Chat](https://platform.openai.com/docs/api-reference/chat)\n",
    "- [Embeddings](https://platform.openai.com/docs/api-reference/embeddings)\n",
    "\n",
    "Why the convergence?\n",
    "- Developer tools (e.g., LangChain, LlamaIndex, OpenRouter) are already built around these formats\n",
    "- Developers don‚Äôt need to learn a new interface for every model (easy model swapping)\n",
    "- It became a de facto \"dialect\" for talking to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22314ca6-1769-43ad-b7f3-ab6ad9f06211",
   "metadata": {},
   "source": [
    "# Using OpenRouter for LLM Access\n",
    "\n",
    "For the rest of this workshop, we‚Äôll be using a service called **OpenRouter** to interact with LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## What is OpenRouter?\n",
    "\n",
    "OpenRouter is a unified API gateway for Large Language Models. It allows you to send a prompt to many different models (like GPT-4, Claude, Mistral, and others) using a single, consistent interface. Though OpenRouter doesn't host the LLM servers themselves, they allow you to interact with LLM hosts through one unified API. \n",
    "\n",
    "In addition to providing one unified experience, we are using OpenRouter since it doesn't require a credit card to interact with the free tier of models (though you are limited to 50 calls per day).\n",
    "\n",
    "## Getting Set Up\n",
    "\n",
    "To use OpenRouter, you‚Äôll need to:\n",
    "1. Create a free account at [https://openrouter.ai](https://openrouter.ai)\n",
    "2. Click on your profile (top right) and generate an API key\n",
    "    - Settings -> API Keys -> Create API Key\n",
    "4. ‚ö†Ô∏è **Warning:** Save this API Key Somewhere Safe\n",
    "    -  It is generally best practice to save this key somewhere secure like an environment variable or in a secure file on your system.<br>\n",
    "    - This is a very sensitive key which allows you to interface directly with the API's. Do not share it with anyone or upload it to any public accounts like Github.\n",
    "    - You will only be able to view this key once, before you can't see it again. If you lose it, you will have to create a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73831f44-76dd-4314-865b-87ff6fb87678",
   "metadata": {},
   "source": [
    "For the purposes of this workshop, let's make a new file in your environment called API_KEY, save it there, and then read our key directly from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200b4ddb-3fb1-4505-ae90-e24b4f54ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in the API_KEY which we've saved to a file\n",
    "with open('API_KEY.txt', 'r') as file:\n",
    "    API_KEY = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c90d0-c6b6-43b8-82f6-2ba3771ee1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI Python Client\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2af2c4b3-b0a4-41b4-a98a-a01703408965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the OpenAI Python Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Notice how we can utilize the OpenAI Client with non OpenAI Hosts/Models due to the unified standard\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\", \n",
    "  api_key=\"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\" # Place API Key Here,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6d002-0606-4375-b77b-ef81df580efb",
   "metadata": {},
   "source": [
    "So now we have our client setup. Next, we need to figure out which model to use. \n",
    "On OpenRouter, you can navigate to [models](https://openrouter.ai/models) tab and search free to view all free models. Notice popular models like Llama 3.2, Google Gemma, and DeepSeek V3. We will choose DeepSeek v3 as it has the largest token set, but feel free to try any other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799643f-4f68-4f42-9c92-7c7968259362",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Be careful not to run these cells too many times though. We are only allowed **50 free calls per day** with the free tier on Open Router. Any more than that, and you will be prevented from making any more requests till the next day. \n",
    "\n",
    "üí° **Tip**: You can check how many requests you've used per day by navigating to the [Activity Menu](https://openrouter.ai/activity) in OpenRouter (Top Left Menu -> Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba134d99-cfa5-41d9-ae64-fa76b668f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a Completion (Output) from the Chat Completions Endpoint\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3.1:free\",\n",
    "    max_tokens=150,\n",
    "    messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Tell me about the D-Lab at UC Berkeley in 50 words or less.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4077e754-1a53-4b70-8725-968d2bfcbb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'annotations': None,\n",
      "                          'audio': None,\n",
      "                          'content': \"UC Berkeley's D-Lab is an \"\n",
      "                                     'interdisciplinary hub offering courses, '\n",
      "                                     'workshops, and consulting. It equips '\n",
      "                                     'students and researchers with practical '\n",
      "                                     'data science and computational skills '\n",
      "                                     'for real-world social science and '\n",
      "                                     'humanities research, fostering '\n",
      "                                     'cutting-edge, data-driven scholarship.',\n",
      "                          'function_call': None,\n",
      "                          'reasoning': None,\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant',\n",
      "                          'tool_calls': None},\n",
      "              'native_finish_reason': 'stop'}],\n",
      " 'created': 1759788622,\n",
      " 'id': 'gen-1759788622-4dXnbek18r0hvvYn0prZ',\n",
      " 'model': 'deepseek/deepseek-chat-v3.1:free',\n",
      " 'object': 'chat.completion',\n",
      " 'provider': 'DeepInfra',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': None,\n",
      " 'usage': {'completion_tokens': 48,\n",
      "           'completion_tokens_details': None,\n",
      "           'prompt_tokens': 20,\n",
      "           'prompt_tokens_details': None,\n",
      "           'total_tokens': 68}}\n"
     ]
    }
   ],
   "source": [
    "# Pretty Printing The Completion Response\n",
    "from pprint import pprint\n",
    "pprint(completion.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce169e4-224a-4811-99f1-6cfc1a107954",
   "metadata": {},
   "source": [
    "Notice the abundance of data contained in the raw Completions Object. This Object represents the response from the LLM featuring the outpue message, content, finish_reason, and a variety of other useful metadata. To get the generated response, we want to focus on the choices object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07536fec-0268-48e7-a07a-35e324f6bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC Berkeley's D-Lab is an interdisciplinary hub offering courses, workshops, and consulting. It equips students and researchers with practical data science and computational skills for real-world social science and humanities research, fostering cutting-edge, data-driven scholarship.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96f9ef-c90e-49bd-9436-9b40c0b3515a",
   "metadata": {},
   "source": [
    "Notice the broad knowledge base of the DeepSeek Model which contains information about UC Berkley's very own D-Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c812746-d3a0-4852-a6e7-950d199770b6",
   "metadata": {},
   "source": [
    "### Congrats! This marks the end of the first part of today's workshop! \n",
    "\n",
    "Now that we've established the 'why,' Part 2 will dive into the 'how.' We will directly interface with the API that powers platforms like ChatGPT, breaking down the fundamental parameters of the API call. After learning these fundamentals, you will be able to use the API to power your own scalable research workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b223f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D_Lab-M2S9YoIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
