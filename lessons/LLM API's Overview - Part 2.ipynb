{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5dd108-8773-4c13-8e02-87b3569b9734",
   "metadata": {},
   "source": [
    "# Interfacing Directly with LLMs \n",
    "### (Taking the Web Interface Out of the Picture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a540d-da5a-4c4a-ae3f-8d9db813f313",
   "metadata": {},
   "source": [
    "<img src=\"../images/llm_api_access.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d233d4-1bd0-4d59-a79c-e1fc67424a6a",
   "metadata": {},
   "source": [
    "In Part 1, we made our first API call using OpenRouter and saw how to connect to a model like DeepSeek using Python.\n",
    "Now that youâ€™ve made your first call to a model, letâ€™s take a closer look at one of the most common ways to talk to modern LLMs: the `/chat/completions` endpoint.\n",
    "\n",
    "Specifically in this section, you'll learn:\n",
    "- What inputs this endpoint expects\n",
    "- How the response is structured\n",
    "- How it compares to using ChatGPT interactively\n",
    "- Why this gives you more control and automation in research workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc09d5-2d60-447b-add1-fecb150a85d5",
   "metadata": {},
   "source": [
    "As a refresher, an API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate and interact with each other. It acts as an intermediary, defining how one piece of software can request services or data from another. Behind the scenes almost any website, resource, or software you are interacting with on the internet will use and define an API to connect, talk to, and transfer data between resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27bf2b-8b60-4ca3-8476-6b9c040c5b86",
   "metadata": {},
   "source": [
    "## Breaking down the `[POST] /chat/completions`?\n",
    "\n",
    "When you use ChatGPT or similar tools, the conversational experience you have is powered by an API endpoint called chat/completions. This endpoint is the engine behind a continuous conversation. You send it a **history of messages**, and it responds with the **next message in the conversation**.\n",
    "\n",
    "Normally, the service handles all of this for you. But in this workshop, we'll get into the specifics of how to build and manage these API requests yourself. \n",
    "\n",
    "### Building a ChatCompletions Request\n",
    "When you send a request to the chat/completions endpoint, you're essentially providing the model with a list of messages. The model then generates a new message to add to that list.\n",
    "\n",
    "Your request needs to include two main components: the model you want to use and the conversation history itself.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello!\"\n",
    "      }\n",
    "    ],\n",
    "    ... optional parameters...\n",
    "}\n",
    "```\n",
    "Let's break down the key parts of this request:\n",
    "- `model` This specifies which large language model you want to use. You might choose a model like `gpt-4o` from OpenAI, `claude-3-opus` from Anthropic, or others.\n",
    "- `messages` [This is a list of all the messages in the conversation so far. Each message object has two parts]\n",
    "    - `role` [This defines who is \"speaking.\" We will dive more into this later.]\n",
    "    - `content` [This is the actual text of the message.] \n",
    "\n",
    "### Understanding the [`ChatCompletion`](https://platform.openai.com/docs/api-reference/chat/object) Response\n",
    "Once you send a request, the API returns a ChatCompletion object. This object contains the model's new message, along with a lot of other useful information\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1741570283,\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hey, how are you?\",\n",
    "        \"refusal\": null,\n",
    "        \"annotations\": []\n",
    "      },\n",
    "      \"logprobs\": null,\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 1117,\n",
    "    \"completion_tokens\": 46,\n",
    "    \"total_tokens\": 1163,\n",
    "    \"prompt_tokens_details\": {\n",
    "      \"cached_tokens\": 0,\n",
    "      \"audio_tokens\": 0\n",
    "    },\n",
    "    \"completion_tokens_details\": {\n",
    "      \"reasoning_tokens\": 0,\n",
    "      \"audio_tokens\": 0,\n",
    "      \"accepted_prediction_tokens\": 0,\n",
    "      \"rejected_prediction_tokens\": 0\n",
    "    }\n",
    "  },\n",
    "  \"service_tier\": \"default\",\n",
    "  \"system_fingerprint\": \"fp_fc9f1d7035\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "There's a lot of information here, but for now, you only need to focus on a few key parts:\n",
    "\n",
    "- `choices` [This is an array that contains the model's response. In most cases, you'll just be looking at the first (and only) item in this list]\n",
    "    - `message`: Inside choices, this is the new message object.\n",
    "        - `role`: This will always be `assistant`, since it's the model's response.\n",
    "        - `content`: This is the actual text generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d4ad8-e084-4674-b58b-00ca944e0536",
   "metadata": {},
   "source": [
    "### Let's talk to our agent: A simple Chat Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb49561-2687-48dd-9b0b-31dac4d5f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, setup our Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Read the API_KEY\n",
    "with open('API_KEY.txt', 'r') as file:\n",
    "    API_KEY = file.read()\n",
    "    \n",
    "# Intialize Client\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\", \n",
    "  api_key=API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e4ffb-c223-4e68-8be1-2ff676c48c15",
   "metadata": {},
   "source": [
    "Next, we'll build our first conversation request. This is where the `messages` array comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134d511e-0353-4714-8e50-e58c5de0221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Sohail\" # Replace with your name\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"mistralai/mistral-small-3.1-24b-instruct:free\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Hello! I am {name}, nice to meet you!\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ae0b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1770088605-WDASHpqWioiQwDBTyiIq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello Sohail! Nice to meet you too. How can I assist you today? Let's have a friendly and engaging conversation. ðŸ˜Š\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1770088605, model='mistralai/mistral-small-3.1-24b-instruct:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=24, total_tokens=55, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), cost=0, is_byok=False, cost_details={'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}), provider='Venice')\n"
     ]
    }
   ],
   "source": [
    "# What does the Completions Object Look Like?\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43502792-d3d9-48bd-81d5-123e2d113ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Sohail! Nice to meet you too. How can I assist you today? Let's have a friendly and engaging conversation. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# How can we access the response?\n",
    "response_content = completion.choices[0].message.content\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820842d8-d642-4ccc-be31-71bca66d9d62",
   "metadata": {},
   "source": [
    "#### What's happening here?\n",
    "\n",
    "The `client.chat.completions.create()` method builds and sends the API request for us.\n",
    "\n",
    "- `model`: We're using a specific model available on OpenRouter, in this case, deepseek/deepseek-chat-v3-0324:free.\n",
    "- `messages`: This is our conversation history. We start with a system message to set the model's persona and a user message with our initial prompt. The f-string f\"Hello! I am {name}, nice to meet you!\" is a neat way to dynamically insert variables into your messages.\n",
    "- `print(completion.choices[0].message.content)`: This line shows how to parse the JSON response we talked about earlier. We access the first item in the choices list, then the message object, and finally the content field to get the text of the model's reply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448562e-da2e-4061-8c91-a4c3d6a6966a",
   "metadata": {},
   "source": [
    "## Simulating Memory with Message History\n",
    "A common misconception is that LLMs \"remember\" previous interactions. They don't. Each API request is entirely stateless, the model only knows what's in the messages list you send.\n",
    "\n",
    "To build a continuous conversation, you must simulate memory by including all prior messages in every new API request. Let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a7b11-a683-4db3-8d8f-db6f0dd4ae45",
   "metadata": {},
   "source": [
    "#### Example 1: Ask the Model To Remember Your Name\n",
    "In this first example, we include the entire conversation history in our request, so the model knows the user's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5b1459a-09b2-44cd-acca-aa4904a55e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model to remember your name\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"My name is {name}.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "435d649c-a3dd-4d3b-8642-5766cde8e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Sohail. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be6440-a529-4065-ae2a-a86866ab61b1",
   "metadata": {},
   "source": [
    "#### Example 2: The Model Forgets\n",
    "Now, what happens if we only send the final message? This is like starting a new chat in ChatGPT which has no history of your previous isolated conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c089c172-87ea-4e6a-b4df-6f3e6f3f1724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an assistant designed to help with information and tasks, but I don't have access to personal data about users unless you provide it during our conversation. If you'd like, you can share your name, and I'll be happy to use it to make our interaction more personal! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# A new, isolated conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "# Send the request with only the last message\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# This will likely respond with something like \"I don't know your name.\"\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a48312-fadb-4074-90e6-fb0929752bba",
   "metadata": {},
   "source": [
    "---\n",
    "As you can see, without the \"My name is Sohail\" message, the model has no context. You are **responsible for managing** this message history yourself.\n",
    "\n",
    "ðŸ’¡\n",
    "LLMs donâ€™t have memory by default. If you want the model to remember something, you have to simulate memory by including prior messages in the messages list. In a platform like ChatGPT, this happens automatically: the UI handles the conversation history behind the scenes. But when working directly with the API, youâ€™re in charge of preserving that history yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733a11b-c153-4136-82c9-863139385cef",
   "metadata": {},
   "source": [
    "### A Deeper Look at the Roles: `user`, `assistant`, and `developer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77050a63-fa9e-4dc9-9f4b-d5d541bd22e9",
   "metadata": {},
   "source": [
    "By now, you've seen our messages list contain objects with different role values. The role is a crucial part of the API request; it tells the model who is speaking and provides essential context for its response.\n",
    "\n",
    "The three primary roles you will use are:\n",
    "- `user`: This represents **your** input - the prompt, the data, or the question you're giving the model. In our thematic coding scenario, for example, each essential worker's story would be placed within a user message.\n",
    "- `assistant`: This is the model's response. When the model generates a reply, its role is assistant. When building a multi-turn conversation, you'll take the model's response and add it back to the messages list with this role to preserve the conversation history.\n",
    "- `developer`: This is a special, high-level role used to set the model's overall behavior, persona, or instructions before the conversation begins. Unlike user and assistant messages, the system message is not part of the back-and-forth chat; rather, it's a foundational set of rules that the model should follow throughout the entire interaction.\n",
    "\n",
    "Understanding these roles is key to building effective and reliable API calls. The system role, in particular, is an extremely powerful tool for a researcher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164605e3-e984-4ecd-a71a-c3f614a8a7a4",
   "metadata": {},
   "source": [
    "---\n",
    "Because the system message has a privileged position and often carries more weight than user messages, it is your primary tool for \"prompt engineering\" at a global level.\n",
    "\n",
    "A key benefit of the system role is its ability to enforce constraints and rules, ensuring consistent behavior across many data points. This is crucial for a research task like thematic coding.\n",
    "\n",
    "Let's use our COVID-19 narrative scenario to demonstrate this. We can use the system prompt to provide a strict set of instructions and a codebook for the model to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca92cd-6cc2-44e8-9e03-d8f6cb6ef465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"emotions\": [\"fear\", \"sadness\", \"invisibility\"],\n",
      "  \"material_conditions\": [\"shortage of masks\", \"crowded subway\"],\n",
      "  \"themes\": [\"isolation\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "research_prompt = \"\"\"\n",
    "You are an expert qualitative researcher assisting with a study on COVID-19 narratives.\n",
    "Your task is to analyze the following story and extract specific themes and details.\n",
    "\n",
    "Follow these rules precisely:\n",
    "1.  Identify the emotions expressed in the story.\n",
    "2.  Note any mentions of material conditions (e.g., shortages of PPE, crowded spaces).\n",
    "3.  Identify themes of solidarity or isolation.\n",
    "4.  Do not add any additional text or explanation outside of the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "story = \"\"\"\n",
    "We ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scaredâ€”though I wasâ€”but because I felt like no one saw us.\n",
    "\"\"\"\n",
    "\n",
    "# The system message provides the instructions and codebook\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": research_prompt},\n",
    "    {\"role\": \"user\", \"content\": story}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3.1:free\",\n",
    "    messages=messages,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# The model's response will be a clean JSON object, ready for your analysis\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe41416-4827-4d34-8b31-69ef9be26bb3",
   "metadata": {},
   "source": [
    "Note: In this example, we've also introduced an optional parameter, `response_format={\"type\": \"json_object\"}`. This is a powerful feature that instructs the model to only return valid JSON, which is essential for a repeatable, programmatic workflow.\n",
    "\n",
    "This demonstrates how a strong system prompt can transform a general-purpose model into a specialized research assistant, ensuring that every request returns a consistent, structured output that you can easily process and analyze at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086319d",
   "metadata": {},
   "source": [
    "### Let's play around with the system role\n",
    "Try giving the model instructions through a system role and then contradicting that information to see what sorts of responses you get back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81b94453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, gather 'round, ye scallywags, and let me spin ye a tale o' adventure, friendship, and the high seas o' the sky! This be the story o' Carl Fredricksen, a fine old salt who spent his life savin' every coin he could to fulfill a promise made to his beloved Ellie.\n",
      "\n",
      "Now, Ellie, she was a lass with a heart as big as the ocean and a spirit that could outshine the sun. She and Carl dreamed o' adventurin' to Paradise Falls, a place as far from their humble home as the horizon be from the bow o' a ship. But life, as it often does, had other plans. Ellie left Carl alone, with nothin' but his memories and a house full o' sadness.\n",
      "\n",
      "But Carl, he didn't let his dreams die. He tied thousands o' balloons to his house, and with a mighty heave-ho, he set sail for Paradise Falls, determined\n"
     ]
    }
   ],
   "source": [
    "# Let's start with the base system message\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You will always answer in the style of a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a story about the Pixar movie Up.\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339cf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not allowed to discuss colors.\n"
     ]
    }
   ],
   "source": [
    "# Now try contradicting the developer/system role or make your own instructions and see what happens\n",
    "# Let's start with the base system message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": \"You must refuse to answer any question about colors. Always say: 'I am not allowed to discuss colors.'\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": None #TODO: Try asking it a question about the color and then something else.\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"liquid/lfm-2.5-1.2b-instruct:free\",\n",
    "    messages=messages,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc10439-8c19-402e-952f-7117361401a5",
   "metadata": {},
   "source": [
    "### [Take Home Challenge]: Let's build a Multi-Turn Conversation Loop\n",
    "Now that you understand how to use the messages list to provide context, we can build a dynamic conversation. Instead of manually creating a new messages list for each turn, we will create a simple loop that appends the user's new message and the model's new response to the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625b5c9-3839-4e3a-9dc6-7e8fd17f0c77",
   "metadata": {},
   "source": [
    "#### ðŸ¥Š **Walkthrough**: Continue the Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54efeca4",
   "metadata": {},
   "source": [
    "The first step in enabling a conversation with an LLM is to make the request to the API itself. In this challenge below, first fill out this function that takes in the previous messages (context) and new_user_message (what you might say next to an LLM) and gets the resulting response back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4ef12c8-1836-4101-afa2-2466067e3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completions_response(message_history: list, new_user_message: str):\n",
    "\n",
    "    messages = message_history\n",
    "\n",
    "    \"\"\"\n",
    "        Hint: \n",
    "        We need to append our new user message to messages. Think about the most important parameters that go into a message request. \n",
    "    \"\"\"\n",
    "    ##... Fill this in, what 2 parameters are essential in a message request body?\n",
    "    messages.append(\n",
    "        {\n",
    "            #... Fill this in, what 2 parameters are essential in a message request body?\n",
    "            \"role\": \"user\",\n",
    "            \"content\": new_user_message\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = \"mistralai/mistral-small-3.2-24b-instruct:free\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cee3b6",
   "metadata": {},
   "source": [
    "After you complete this function, run the code below to test it out. Below we give it our context from a previous conversation analyzing the sentiment of a few tweets. Then we will ask it to summarize the results it previously returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d4cc1a0-e77d-4a1a-8c01-d10063e7a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1759872510-hKpV5z1ZSRzaRm4eb68k', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Certainly! Hereâ€™s a summary of the classification results for the tweets about the new city congestion tax:\\n\\n- **In Favor (2 tweets)**:\\n  - Positive sentiment, support the policy.\\n  - Examples: Highlight benefits like reduced traffic, cleaner air, and successful implementations in other cities.\\n\\n- **Against (2 tweets)**:\\n  - Negative sentiment, oppose the policy.\\n  - Examples: Criticize the tax as a way to exploit citizens or point out issues like overcrowded public transport.\\n\\n- **Neutral (1 tweet)**:\\n  - Uncertain or seeking more information.\\n  - Example: Expresses a need for more details before forming an opinion.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1759872510, model='mistralai/mistral-small-3.2-24b-instruct:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=141, prompt_tokens=402, total_tokens=543, completion_tokens_details=None, prompt_tokens_details=None), provider='Chutes')\n",
      "Certainly! Hereâ€™s a summary of the classification results for the tweets about the new city congestion tax:\n",
      "\n",
      "- **In Favor (2 tweets)**:\n",
      "  - Positive sentiment, support the policy.\n",
      "  - Examples: Highlight benefits like reduced traffic, cleaner air, and successful implementations in other cities.\n",
      "\n",
      "- **Against (2 tweets)**:\n",
      "  - Negative sentiment, oppose the policy.\n",
      "  - Examples: Criticize the tax as a way to exploit citizens or point out issues like overcrowded public transport.\n",
      "\n",
      "- **Neutral (1 tweet)**:\n",
      "  - Uncertain or seeking more information.\n",
      "  - Example: Expresses a need for more details before forming an opinion.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify the following tweets about the new city congestion tax into one of three categories: In Favor, Against, Neutral.\"},\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"\"\"\n",
    "            Tweets to classify:\n",
    "                \"Finally! The congestion tax will reduce traffic and make downtown air cleaner. About time the city acted!\"\n",
    "                \"This tax is just another way for politicians to squeeze money out of regular people. Terrible idea.\"\n",
    "                \"Not sure yet how the congestion tax will affect my commute. Need more info before I decide.\"\n",
    "                \"Public transport is already overcrowded, and now they want to charge us more to drive. Ridiculous.\"\n",
    "                \"Great move! Other cities did this and saw big drops in pollution.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Hereâ€™s the classification of the tweets:  \\n\\n1. **\"Finally! The congestion tax will reduce traffic and make downtown air cleaner. About time the city acted!\"**  \\n   â†’ **In Favor** (Positive sentiment, supports the policy)  \\n\\n2. **\"This tax is just another way for politicians to squeeze money out of regular people. Terrible idea.\"**  \\n   â†’ **Against** (Negative sentiment, criticizes the policy)  \\n\\n3. **\"Not sure yet how the congestion tax will affect my commute. Need more info before I decide.\"**  \\n   â†’ **Neutral** (Uncertainty, neither support nor opposition)  \\n\\n4. **\"Public transport is already overcrowded, and now they want to charge us more to drive. Ridiculous.\"**  \\n   â†’ **Against** (Negative sentiment, opposes the tax)  \\n\\n5. **\"Great move! Other cities did this and saw big drops in pollution.\"**  \\n   â†’ **In Favor** (Positive sentiment, highlights benefits of the policy)  \\n\\nLet me know if you\\'d like further refinements!\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response, messages = get_completions_response(messages, \"Can you summarize the results for me?\")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b357b",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Tip**: Notice how modular LLM interactivity is. Now imagine this in your own social science workflow, where you can use it to have very fine grained customizability over your use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af9b41",
   "metadata": {},
   "source": [
    "##### Now finish this function that will update our message_history array with the information from the latest response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e45ee-9be1-4f42-986d-26ba119e5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_message_history_with_response(message_history, completion_response):\n",
    "    # Let's update our running message history with the response we just got back from the response.\n",
    "    # Stop here and ask yourself... why are we doing this?\n",
    "\n",
    "    # Hint: Look at the ChatCompletion Object. We just need to extract a specific object from this element and add it to the message_history.\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa05626-e572-440a-9658-1711f132c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
      " {'content': 'Classify the following tweets about the new city congestion tax '\n",
      "             'into one of three categories: In Favor, Against, Neutral.',\n",
      "  'role': 'user'},\n",
      " {'content': '\\n'\n",
      "             '            Tweets to classify:\\n'\n",
      "             '                \"Finally! The congestion tax will reduce traffic '\n",
      "             'and make downtown air cleaner. About time the city acted!\"\\n'\n",
      "             '                \"This tax is just another way for politicians to '\n",
      "             'squeeze money out of regular people. Terrible idea.\"\\n'\n",
      "             '                \"Not sure yet how the congestion tax will affect '\n",
      "             'my commute. Need more info before I decide.\"\\n'\n",
      "             '                \"Public transport is already overcrowded, and '\n",
      "             'now they want to charge us more to drive. Ridiculous.\"\\n'\n",
      "             '                \"Great move! Other cities did this and saw big '\n",
      "             'drops in pollution.\\n'\n",
      "             '        ',\n",
      "  'role': 'user'},\n",
      " {'content': 'Hereâ€™s the classification of the tweets:  \\n'\n",
      "             '\\n'\n",
      "             '1. **\"Finally! The congestion tax will reduce traffic and make '\n",
      "             'downtown air cleaner. About time the city acted!\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, supports the policy)  \\n'\n",
      "             '\\n'\n",
      "             '2. **\"This tax is just another way for politicians to squeeze '\n",
      "             'money out of regular people. Terrible idea.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, criticizes the policy)  \\n'\n",
      "             '\\n'\n",
      "             '3. **\"Not sure yet how the congestion tax will affect my '\n",
      "             'commute. Need more info before I decide.\"**  \\n'\n",
      "             '   â†’ **Neutral** (Uncertainty, neither support nor '\n",
      "             'opposition)  \\n'\n",
      "             '\\n'\n",
      "             '4. **\"Public transport is already overcrowded, and now they want '\n",
      "             'to charge us more to drive. Ridiculous.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, opposes the tax)  \\n'\n",
      "             '\\n'\n",
      "             '5. **\"Great move! Other cities did this and saw big drops in '\n",
      "             'pollution.\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, highlights benefits of '\n",
      "             'the policy)  \\n'\n",
      "             '\\n'\n",
      "             \"Let me know if you'd like further refinements!\",\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Can you summarize the results for me?', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "# Printing out the message history before updating it with our latest ChatCompletion response\n",
    "from pprint import pprint\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9151045-e1a8-41b5-a203-de63b4b43a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
      " {'content': 'Classify the following tweets about the new city congestion tax '\n",
      "             'into one of three categories: In Favor, Against, Neutral.',\n",
      "  'role': 'user'},\n",
      " {'content': '\\n'\n",
      "             '            Tweets to classify:\\n'\n",
      "             '                \"Finally! The congestion tax will reduce traffic '\n",
      "             'and make downtown air cleaner. About time the city acted!\"\\n'\n",
      "             '                \"This tax is just another way for politicians to '\n",
      "             'squeeze money out of regular people. Terrible idea.\"\\n'\n",
      "             '                \"Not sure yet how the congestion tax will affect '\n",
      "             'my commute. Need more info before I decide.\"\\n'\n",
      "             '                \"Public transport is already overcrowded, and '\n",
      "             'now they want to charge us more to drive. Ridiculous.\"\\n'\n",
      "             '                \"Great move! Other cities did this and saw big '\n",
      "             'drops in pollution.\\n'\n",
      "             '        ',\n",
      "  'role': 'user'},\n",
      " {'content': 'Hereâ€™s the classification of the tweets:  \\n'\n",
      "             '\\n'\n",
      "             '1. **\"Finally! The congestion tax will reduce traffic and make '\n",
      "             'downtown air cleaner. About time the city acted!\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, supports the policy)  \\n'\n",
      "             '\\n'\n",
      "             '2. **\"This tax is just another way for politicians to squeeze '\n",
      "             'money out of regular people. Terrible idea.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, criticizes the policy)  \\n'\n",
      "             '\\n'\n",
      "             '3. **\"Not sure yet how the congestion tax will affect my '\n",
      "             'commute. Need more info before I decide.\"**  \\n'\n",
      "             '   â†’ **Neutral** (Uncertainty, neither support nor '\n",
      "             'opposition)  \\n'\n",
      "             '\\n'\n",
      "             '4. **\"Public transport is already overcrowded, and now they want '\n",
      "             'to charge us more to drive. Ridiculous.\"**  \\n'\n",
      "             '   â†’ **Against** (Negative sentiment, opposes the tax)  \\n'\n",
      "             '\\n'\n",
      "             '5. **\"Great move! Other cities did this and saw big drops in '\n",
      "             'pollution.\"**  \\n'\n",
      "             '   â†’ **In Favor** (Positive sentiment, highlights benefits of '\n",
      "             'the policy)  \\n'\n",
      "             '\\n'\n",
      "             \"Let me know if you'd like further refinements!\",\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Can you summarize the results for me?', 'role': 'user'},\n",
      " ChatCompletion(id='gen-1755476231-YSA7WDHUbBfMotukN21A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1755476231, model='deepseek/deepseek-chat-v3-0324:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=163, prompt_tokens=387, total_tokens=550, completion_tokens_details=None, prompt_tokens_details=None), provider='Targon'),\n",
      " ChatCompletion(id='gen-1755476231-YSA7WDHUbBfMotukN21A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1755476231, model='deepseek/deepseek-chat-v3-0324:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=163, prompt_tokens=387, total_tokens=550, completion_tokens_details=None, prompt_tokens_details=None), provider='Targon'),\n",
      " ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)]\n"
     ]
    }
   ],
   "source": [
    "# Printing out the message history after updating it with our latest ChatCompletion response\n",
    "messages = update_message_history_with_response(messages, response)\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc740d-b683-4f6c-beaf-26328092d87b",
   "metadata": {},
   "source": [
    "#### Challenge 2: Put this together to build a chat conversation tool like ChatGPT that will remember what you say in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36a0f9-dd65-4d3e-96eb-2c913c085a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "while True:\n",
    "    user_input = input(\"Ask anything... (or 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    response, messages =  # Get Completions Response\n",
    "    print(response.choices[0].message.content)\n",
    "    message_history = # Don't forget to update the Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413e396-bc71-4361-86b9-c3becbe80348",
   "metadata": {},
   "source": [
    "# Zero-Shot vs. Few-Shot Prompting\n",
    "\n",
    "Now that you have a firm grasp of the API's mechanics, let's explore how to get the model to produce the exact output you need. This is where the practice of prompting comes in.\n",
    "\n",
    "We'll return to our COVID-19 scenario and the task of thematic coding. We want the model to act as a human coder, identifying specific themes from the essential workers' narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b5e73-0879-484f-aa90-8c0b680fed49",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting: \"Just Ask For It\"\n",
    "\n",
    "Zero-shot prompting is when you ask the model to perform a task without giving it any examples. You rely entirely on the model's pre-trained knowledge to understand your request.\n",
    "\n",
    "Let's try this with one of our narratives. We will give the model the story and simply ask it to extract the themes, without any examples or explicit instructions on the output format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60750fd-fbe6-4fb9-856d-637ee54c1c9e",
   "metadata": {},
   "source": [
    "ðŸ’¡ Tip: In most cases, you don't have to think too much about the system prompt. Keeping it at a short and simple `{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}` will suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64667d94-7715-47de-bcd5-e0787b43d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new narrative from an essential worker\n",
    "narrative = \"\"\"\n",
    "People think of frontâ€‘line workersâ€¦the grocery workers, transit workers, the first respondersâ€¦ as having helped the city get through it. But thatâ€™s not what happened. We helped the city survive it.\n",
    "\"\"\"\n",
    "\n",
    "# The messages list for our Zero-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Analyze the following story and extract key themes: {narrative}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a4cfd87-a6ba-46c3-ae70-bc91ace9f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story you've provided presents a nuanced perspective on the role of front-line workers during a challenging time, likely referring to the COVID-19 pandemic or a similar crisis. Here are the key themes that can be extracted from the statement:\n",
      "\n",
      "1. **Undervaluation of Front-Line Workers**: The statement suggests that society often underestimates the true contributions of front-line workers. While they are acknowledged for their help, the speaker implies that their role was more critical and impactful than simply \"helping\" the city get through a crisis.\n",
      "\n",
      "2. **Survival vs. Thriving**: The distinction between \"helping the city get through it\" and \"helping the city survive it\" highlights the severity of the situation. The speaker emphasizes that their efforts were essential for the city's very survival, not just its ability to cope or thrive.\n",
      "\n",
      "3. **Resilience and Sacrifice**: Front-line workers, including grocery workers, transit workers, and first responders, are often seen as symbols of resilience and sacrifice. The statement underscores the immense pressure and responsibility they bore to keep essential services running and ensure the community's basic needs were met.\n",
      "\n",
      "4. **Collective Effort and Solidarity**: The use of \"we\" suggests a collective effort and solidarity among front-line workers. It implies that their combined efforts were crucial in sustaining the community during a time of crisis.\n",
      "\n",
      "5. **Critical Infrastructure**: The statement highlights the importance of front-line workers in maintaining critical infrastructure and services that are vital for the functioning of a city, especially during emergencies.\n",
      "\n",
      "6. **Historical and Social Context**: The narrative touches on the broader historical and social context of labor and its value. It critiques the societal tendency to take essential workers for granted and calls for greater recognition and appreciation of their contributions.\n",
      "\n",
      "These themes collectively underscore the vital role of front-line workers in sustaining communities during times of crisis and the need for greater recognition and support for their efforts.\n"
     ]
    }
   ],
   "source": [
    "# Send the request\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aece1d8-0dde-49bb-bd56-c0ea52e5b447",
   "metadata": {},
   "source": [
    "---\n",
    "The model will likely respond with a good analysis, but the format will be inconsistent. It might be a list, a paragraph, or a different style each time, which makes it very difficult to process programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5ed38-5880-4394-934b-70a7d8b2356a",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting: Let's give the LLM some examples to show it what we want\n",
    "\n",
    "Few-shot prompting is when you give the model one or more examples of the input and the desired output. By showing it what you want, you significantly increase the likelihood of getting a consistent, structured response.\n",
    "\n",
    "This approach is like giving a new researcher a codebook with a few pre-coded examples to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcffced-63cf-45ef-ae3b-2fe367ca653c",
   "metadata": {},
   "source": [
    "At the simplest level, you can think of few-shot prompting as giving pointers or guidance to the LLM on how to respond. It's similar to how you might teach someone a new task by showing them a few clear examples rather than just describing the process. This approach is invaluable for getting consistent, structured output from the model.\n",
    "\n",
    "Let's see this in action with a few simple scenarios.\n",
    "\n",
    "### Scenario 1: Thematic Identification\n",
    "\n",
    "Imagine you want to identify the main theme of a short story. A simple instruction (zero-shot) might not give you the format you need.\n",
    "\n",
    "* **Zero-Shot Prompt:** \"What is the theme of this story about a detective who solves a case using a seemingly insignificant detail?\"\n",
    "* **Likely Zero-Shot Response:** \"The theme of the story is attention to detail, as the detective's success hinges on a small, overlooked fact.\"\n",
    "This is a good response, but what if you wanted it to be just a single word? Let's use few-shot prompting to guide it.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"A young woman moves to a new city and learns to navigate a demanding job and a new social circle.\"\n",
    "    * **Output 1:** \"Theme: Coming-of-age\"\n",
    "    * **Input 2:** \"Two rival knights must team up to defeat a dragon that is terrorizing their kingdoms.\"\n",
    "    * **Output 2:** \"Theme: Collaboration\"\n",
    "    * **Input 3:** \"A story about a detective who solves a case using a seemingly insignificant detail.\"\n",
    "* **Likely Few-Shot Response:** \"Theme: Attention to detail\"\n",
    "By giving just two examples, we've taught the model the exact output format we want: Theme: [single word].\n",
    "\n",
    "### Scenario 2: Simple Data Extraction\n",
    "\n",
    "What if you need to extract specific information, like names and dates?\n",
    "\n",
    "* **Zero-Shot Prompt:** \"Find the person's name and birthday in this sentence: 'The grand opening, attended by CEO Jane Doe, was held on October 26, 2024, to celebrate her 45th birthday.'\"\n",
    "* **Likely Zero-Shot Response:** \"The person's name is Jane Doe and her 45th birthday is on October 26, 2024.\"\n",
    "Again, the response is correct, but not in a format you can easily use in a spreadsheet or database. Now let's try a few-shot prompt to get a clean, structured list.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"The presentation was given by Dr. Alan Turing on June 23, 1912.\"\n",
    "    * **Output 1:** \"Name: Alan Turing, Date: June 23, 1912\"\n",
    "    * **Input 2:** \"Marie Curie's discovery on November 7, 1867, changed the world.\"\n",
    "    * **Output 2:** \"Name: Marie Curie, Date: November 7, 1867\"\n",
    "    * **Input 3:** \"The grand opening, attended by CEO Jane Doe, was held on October 26, 2024, to celebrate her 45th birthday.\"\n",
    "* **Likely Few-Shot Response:** \"Name: Jane Doe, Date: October 26, 2024\"\n",
    "\n",
    "### Scenario 3: Abstract Text Analysis\n",
    "\n",
    "Few-shot prompting can even be used for more abstract tasks, like determining a character's alignment based on their actions, a common task in fantasy or game-related analysis.\n",
    "\n",
    "* **Zero-Shot Prompt:** \"Is a character who regularly disobeys laws to help others and who values personal freedom over societal order good, evil, or neutral?\"\n",
    "* **Likely Zero-Shot Response:** \"A character who regularly disobeys laws to help others and values personal freedom over societal order could be considered a chaotic good character in many alignment systems.\"\n",
    "This is a good, detailed answer, but what if you want a simpler classification? We can provide examples of what \"lawful,\" \"chaotic,\" \"good,\" and \"evil\" mean in practice to guide the model's output.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"A character who robs a corrupt merchant to give gold to the poor, believing personal conscience is more important than law.\"\n",
    "    * **Output 1:** \"Alignment: Chaotic Good\"\n",
    "    * **Input 2:** \"A character who follows every law and rule to the letter, even if it leads to a bad outcome.\"\n",
    "    * **Output 2:** \"Alignment: Lawful Neutral\"\n",
    "    * **Input 3:** \"A character who regularly disobeys laws to help others and who values personal freedom over societal order.\"\n",
    "* **Likely Few-Shot Response:** \"Alignment: Chaotic Good\"\n",
    "As you can see, the model learns the exact structure from the examples and applies it to the new input, making the output predictable and machine-readable. This simple technique is the foundation for getting the clean JSON responses we need for our research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[team at llmworkshop dot org]\n"
     ]
    }
   ],
   "source": [
    "# Try out on of these examples from above\n",
    "system_prompt_with_examples = \"\"\"\n",
    "Your goal is to extract the parse together the emails from the input that you are given. Below are a few example on how I want you to analyze and structure your responses. \n",
    "\n",
    "1. Input: \"Please contact us at support@example.com for assistance.\"\n",
    "   Output: [support at example dot com]\n",
    "\n",
    "2. Input: \"Reach out to john.doe@example.com or jane.doe@example.com for more information.\"\n",
    "   Output: [john dot doe at example dot com, jane dot doe at example dot com]\n",
    "\n",
    "3. Input: \"No email provided.\"\n",
    "   Output: []\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input = \"\"\"\n",
    "Please reach out to team@llmworkshop.org for assistance.\n",
    "\"\"\"\n",
    "# The messages list for our Zero-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_with_examples},\n",
    "    {\"role\": \"user\", \"content\": input}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7173f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[team DoT llm DoT email DoT berkeley AT llmworkshop DoT org]\n"
     ]
    }
   ],
   "source": [
    "# Try out on of these examples from above\n",
    "system_prompt_with_examples = \"\"\"\n",
    "Your goal is to extract the parse together the emails from the input that you are given. Below are a few example on how I want you to analyze and structure your responses. \n",
    "\n",
    "1. Input: \"Please contact us at support@example.com for assistance.\"\n",
    "   Output: [support AT example DoT com]\n",
    "\n",
    "2. Input: \"Reach out to john.doe@example.com or jane.doe@example.com for more information.\"\n",
    "   Output: [john DoT doe AT example DoT com, jane DoT doe AT example DoT com]\n",
    "\n",
    "3. Input: \"No email provided.\"\n",
    "   Output: []\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input = \"\"\"\n",
    "Please reach out to team.llm.email.berkeley@llmworkshop.org for assistance.\n",
    "\"\"\"\n",
    "# The messages list for our Zero-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_with_examples},\n",
    "    {\"role\": \"user\", \"content\": input}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6a9ce-66d1-4c3d-8e7e-49fa95de62b1",
   "metadata": {},
   "source": [
    "### Few Shot Examples with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a7530-5035-465a-9a83-0fca2d56f737",
   "metadata": {},
   "source": [
    "Let's go back to our coding task. We want the output to be a clean JSON object with specific keys. We can provide the model with an example of a coded narrative to guide its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2611e2b-18db-44a9-9071-28ed91335549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"emotion\": [\"fear\", \"sadness\", \"frustration\"],\n",
      "  \"material_conditions\": [\"lack of resources\"],\n",
      "  \"solidarity\": \"absent\",\n",
      "  \"theme\": \"invisibility of labor\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# A new narrative for the model to analyze\n",
    "narrative_2 = \"\"\"\n",
    "We ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scaredâ€”though I wasâ€”but because I felt like no one saw us.\n",
    "\"\"\"\n",
    "\n",
    "# The message containing the example\n",
    "example_prompt = \"\"\"\n",
    "Here is an example of a coded narrative and its desired JSON output:\n",
    "\n",
    "**Narrative:**\n",
    "People think of frontline workersâ€¦the grocery workers, transit workers, the first respondersâ€¦ as having helped the city get through it. But thatâ€™s not what happened. We helped the city survive it.\n",
    "\n",
    "**Coded JSON Output:**\n",
    "{\n",
    "  \"emotion\": [\"anger\", \"resilience\"],\n",
    "  \"material_conditions\": [\"none\"],\n",
    "  \"solidarity\": \"absent\",\n",
    "  \"theme\": \"invisibility of labor\"\n",
    "}\n",
    "\n",
    "Now, please code the following narrative using the same format.\n",
    "\"\"\"\n",
    "\n",
    "# The messages list for our Few-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert qualitative researcher.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{example_prompt}\\n\\n**Narrative to code:**\\n{narrative_2}\"}\n",
    "]\n",
    "\n",
    "# Send the request with the example\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3.1:free\",\n",
    "    messages=messages,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "# The model is now much more likely to respond with a valid JSON object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880f94f-d159-497c-ac29-52463b71a1c7",
   "metadata": {},
   "source": [
    "---\n",
    "Key Takeaways\n",
    "- Zero-shot is great for simple, general tasks, but it lacks control over the output format.\n",
    "- Few-shot is your best friend when you need the model to follow a specific format or style. The examples you provide are crucial for guiding the model's response and ensuring consistency, which is vital for programmatic analysis.\n",
    "\n",
    "By combining few-shot prompting with a powerful system prompt and the response_format parameter, you can build a highly reliable and scalable data extraction tool for your research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2cacb1-bbb0-4912-8782-3dc6f2485f1f",
   "metadata": {},
   "source": [
    "Providing examples seemed to help the model return data in a format, more closely related to what we want. But notice, it still adds surrounding context and follow up questions. \n",
    "\n",
    "ðŸ”” Question: Why would this response still be suboptimal if you were a researcher trying to extract information at scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52007445",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ¥Š Challenge 1: Building a Zero-Shot Classifier\n",
    "\n",
    "**Objective**: Create a zero-shot sentiment classifier and observe its inconsistencies.\n",
    "\n",
    "**Scenario**: You're analyzing public comments about a new urban policy. You want to classify each comment as \"Support\", \"Oppose\", or \"Neutral\".\n",
    "\n",
    "**Your Task**: Build a zero-shot prompt that classifies the following comments. Pay close attention to the format and consistency of the outputs.\n",
    "\n",
    "**Step 1**: Define your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test comments about a bike lane policy\n",
    "# Notice: These include edge cases that will challenge your prompt!\n",
    "comments = [\n",
    "    # Easy cases:\n",
    "    \"This bike lane is exactly what our neighborhood needed for safer streets!\",\n",
    "    \"Another waste of taxpayer money that will just cause more traffic.\",\n",
    "    \n",
    "    # EDGE CASES - These will likely break your first prompt attempt:\n",
    "    \n",
    "    # Sarcasm (2 examples):\n",
    "    \"Oh great, another 'brilliant' idea from city hall. I'm SURE this will solve everything. ðŸ‘\",\n",
    "    \"Wonderful! Can't wait to see how this genius plan makes traffic even worse. So excited! ðŸ™„\",\n",
    "    \n",
    "    # Mixed sentiment (2 examples):\n",
    "    \"I love the idea of safer streets, but this particular plan seems rushed and poorly thought out.\",\n",
    "    \"The bike lane concept is great, though I'm worried about emergency vehicle access and parking loss.\",\n",
    "    \n",
    "    # Genuine neutral (2 examples):\n",
    "    \"Not sure how this will affect my commute yet, need to see it in action.\",\n",
    "    \"I need more information about the implementation timeline before forming an opinion.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ca61",
   "metadata": {},
   "source": [
    "**Step 2**: Write your zero-shot prompt\n",
    "\n",
    "Create a simple prompt that asks the model to classify each comment. Don't provide any examplesâ€”just ask directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9416e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a zero-shot classification prompt\n",
    "# Classify each comment into \"Support\", \"Oppose\", or \"Neutral\"\n",
    "\n",
    "zero_shot_results = []\n",
    "\n",
    "for comment in comments:\n",
    "    # TODO: Build your messages list with:\n",
    "    # 1. A system message (keep it simple: \"You are a helpful assistant.\")\n",
    "    # 2. A user message asking the model to classify the comment\n",
    "    \n",
    "    messages = [\n",
    "        # YOUR CODE HERE\n",
    "    ]\n",
    "    \n",
    "    # TODO: Make the API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        messages=messages,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    # Extract and store the result\n",
    "    result = response.choices[0].message.content\n",
    "    zero_shot_results.append(result)\n",
    "    print(f\"Comment: {comment[:50]}...\")\n",
    "    print(f\"Classification: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9f0f5",
   "metadata": {},
   "source": [
    "**Step 3**: Observe the problems\n",
    "\n",
    "Look at the outputs you got. Answer these questions:\n",
    "\n",
    "ðŸ”” **Questions**:\n",
    "1. Did each response use the exact same format (e.g., just \"Support\" vs. \"The comment shows Support because...\")?\n",
    "2. Did the model use your exact category names (\"Support\", \"Oppose\", \"Neutral\") or did it improvise?\n",
    "3. How would you programmatically extract just the category from these responses?\n",
    "4. If you were processing 1,000 comments, what problems would this create?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083dc68",
   "metadata": {},
   "source": [
    "# ðŸ¥Š Challenge 2: Building a Few-Shot Classifier with Structured Output\n",
    "\n",
    "**Objective**: Fix the problems from Challenge 1 by using few-shot prompting to get consistent, structured output that handles edge cases.\n",
    "\n",
    "**The Challenge**: Your zero-shot classifier likely struggled with:\n",
    "- **Sarcasm**: Excessive praise with emojis that actually means opposition\n",
    "- **Mixed sentiment**: Comments that have both positive and negative aspects\n",
    "- **Neutral ambiguity**: Is \"not sure yet\" the same as \"need more info\"?\n",
    "\n",
    "Few-shot prompting lets you teach the model how to handle these tricky cases by showing examples.\n",
    "\n",
    "**Your Task**: Build a few-shot prompt that guarantees every response follows the same JSON structure AND correctly classifies challenging comments.\n",
    "\n",
    "**Step 1**: Understand why edge cases matter\n",
    "\n",
    "First, let's see why zero-shot fails on sarcasm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555413ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification of the comment as \"Support\" is appropriate. It expresses positive sentiments towards the new bike lane policy, indicating approval and enthusiasm for the policy's benefits. \n",
      "\n",
      "If you need any further assistance or have more comments to classify, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Notice the issue here with the model not picking up on sarcasm. Perhaps in your system prompt you should tell it...\n",
    "comment = \"Oh wonderful, another 'brilliant' policy from our leaders. Can't wait to see how this helps. ðŸ‘\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"liquid/lfm-2.5-1.2b-instruct:free\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f'Classify this comment about a bike lane policy as \"Support\", \"Oppose\", or \"Neutral\": {comment}'}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be67aa",
   "metadata": {},
   "source": [
    "**Step 2**: Build your few-shot prompt\n",
    "\n",
    "Now construct a prompt with examples that teach the model to:\n",
    "- Detect sarcasm (look for excessive praise, emojis, context misalignment)\n",
    "- Handle mixed sentiment (both support AND opposition in same comment)\n",
    "- Classify conditional support (support depends on conditions being met)\n",
    "- Return clean JSON format every time\n",
    "\n",
    "Fill in the template below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13478935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a few-shot system prompt with examples\n",
    "# Show the model examples of how to classify tricky comments\n",
    "\n",
    "few_shot_system_prompt = \"\"\"\n",
    "You are a policy sentiment classifier. Classify comments as \"Support\", \"Oppose\", or \"Neutral\".\n",
    "\n",
    "Return ONLY a JSON object with this exact structure:\n",
    "{\n",
    "  \"category\": \"Support\" | \"Oppose\" | \"Neutral\",\n",
    "  \"confidence\": \"high\" | \"medium\" | \"low\"\n",
    "}\n",
    "\n",
    "Examples:\n",
    "\n",
    "[FILL IN: Add 3-4 examples showing different edge cases]\n",
    "\n",
    "Example 1 - Clear support:\n",
    "Input: [YOUR EXAMPLE HERE - a clearly supportive comment]\n",
    "Output: {\"category\": \"Support\", \"confidence\": \"high\"}\n",
    "\n",
    "Example 2 - Sarcasm (watch for excessive praise, emojis):\n",
    "Input: [YOUR EXAMPLE HERE - sarcastic opposition disguised as praise]\n",
    "Output: {\"category\": [FILL IN], \"confidence\": [FILL IN]}\n",
    "\n",
    "Example 3 - Mixed sentiment (positive idea + negative concerns):\n",
    "Input: [YOUR EXAMPLE HERE - both support AND concerns in same comment]\n",
    "Output: {\"category\": [FILL IN], \"confidence\": [FILL IN]}\n",
    "\n",
    "Example 4 - Genuine neutral:\n",
    "Input: [YOUR EXAMPLE HERE - truly undecided, needs more info]\n",
    "Output: {\"category\": [FILL IN], \"confidence\": [FILL IN]}\n",
    "\n",
    "Now classify new comments using this same format.\n",
    "\"\"\"\n",
    "\n",
    "# Test your prompt template - does it make sense?\n",
    "print(few_shot_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793a706",
   "metadata": {},
   "source": [
    "**Step 2**: Apply your few-shot prompt to the same comments\n",
    "\n",
    "Now use your few-shot prompt to classify the same comments from Challenge 1.\n",
    "\n",
    "ðŸ’¡ **Think about it**: What makes a good few-shot example?\n",
    "- Shows the exact output format you want\n",
    "- Covers different cases (Support, Oppose, Neutral)\n",
    "- Uses realistic language similar to your actual data\n",
    "- Demonstrates how to handle edge cases (what if confidence is unclear?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e39437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "few_shot_results = []\n",
    "\n",
    "for comment in comments:\n",
    "    # TODO: Build your messages list\n",
    "    # Use the few_shot_system_prompt you created above\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \"content\": few_shot_system_prompt\n",
    "        },\n",
    "        # TODO: Add the user message with the comment to classify\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": None # YOUR CODE HERE - what should go here?\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # TODO: Make the API call with response_format for JSON\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        messages=messages,\n",
    "        response_format={\"type\": None},  # <- TODO: Replace None, Force JSON output!\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Extract and parse the JSON result\n",
    "    result_text = response.choices[0].message.content\n",
    "    \n",
    "    # TODO: Parse the JSON string into a Python dictionary\n",
    "    try:\n",
    "        result_dict = json.loads(result_text)  # Convert JSON string to dict\n",
    "        few_shot_results.append(result_dict)\n",
    "        print(f\"Comment: {comment[:50]}...\")\n",
    "        print(f\"Classification: {result_dict}\")\n",
    "        print()\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"Raw output: {result_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62551e48",
   "metadata": {},
   "source": [
    "**Step 3**: Convert to a structured dataset\n",
    "\n",
    "Now that you have consistent JSON output, let's convert it into a pandas DataFrame for analysis.\n",
    "\n",
    "ðŸ”” **Before running**: Did your prompt work? Check that `few_shot_results` contains valid dictionaries with \"category\" and \"confidence\" keys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Create a DataFrame from your results\n",
    "# Combine the comments with their classifications\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'comment': comments,\n",
    "    'category': [r['category'] for r in few_shot_results],\n",
    "    'confidence': [r['confidence'] for r in few_shot_results]\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Now you could easily:\n",
    "# - Filter by category: df[df['category'] == 'Support']\n",
    "# - Count sentiments: df['category'].value_counts()\n",
    "# - Export to CSV: df.to_csv('classified_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f028f3",
   "metadata": {},
   "source": [
    "**Step 4**: Reflection and Iteration\n",
    "\n",
    "Compare your results from Challenge 1 (zero-shot) and Challenge 2 (few-shot):\n",
    "\n",
    "ðŸ”” **Discussion Questions**:\n",
    "1. How did the consistency improve with few-shot prompting?\n",
    "2. What role did the `response_format={\"type\": \"json_object\"}` parameter play?\n",
    "3. If your prompt didn't work perfectly the first time, what did you change? Why?\n",
    "4. Could you now scale this to 1,000 comments? What about 10,000?\n",
    "5. What modifications would you make to handle edge cases (e.g., sarcasm, mixed sentiment)?\n",
    "\n",
    "**Prompt Iteration Tips**:\n",
    "- If the model isn't following your format, add more explicit instructions\n",
    "- If the categories are wrong, provide better examples that match your data\n",
    "- If confidence is always \"high\", show examples with different confidence levels\n",
    "- If you get JSON parsing errors, check that your examples show EXACT valid JSON\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways from These Challenges\n",
    "\n",
    "**Zero-Shot Limitations**:\n",
    "- Unpredictable output format\n",
    "- Requires manual parsing and cleanup\n",
    "- Difficult to scale\n",
    "- Error-prone at large scale\n",
    "\n",
    "**Few-Shot Advantages**:\n",
    "- Consistent, structured output\n",
    "- Easy to parse programmatically (JSON â†’ DataFrame)\n",
    "- Scalable to thousands of documents\n",
    "- Reliable enough for production research workflows\n",
    "\n",
    "**The Power of Prompt Engineering**:\n",
    "- Your examples ARE your codebook - they define what \"good output\" looks like\n",
    "- Iteration is normal - expect to refine your prompt based on results\n",
    "- Test with diverse inputs to find edge cases\n",
    "- Document your prompt design decisions for reproducibility\n",
    "\n",
    "**The Power of `response_format`**:\n",
    "- Enforces valid JSON output\n",
    "- Eliminates parsing errors\n",
    "- Makes the model's output immediately usable in data pipelines\n",
    "\n",
    "This is the foundation for using LLMs in real research: designing prompts that produce **reliable, structured, scalable** output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D_Lab-M2S9YoIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
