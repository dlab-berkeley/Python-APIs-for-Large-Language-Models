{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5dd108-8773-4c13-8e02-87b3569b9734",
   "metadata": {},
   "source": [
    "# Interfacing Directly with LLMs \n",
    "### (Taking the Web Interface Out of the Picture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a540d-da5a-4c4a-ae3f-8d9db813f313",
   "metadata": {},
   "source": [
    "<img src=\"../images/llm_api_access.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d233d4-1bd0-4d59-a79c-e1fc67424a6a",
   "metadata": {},
   "source": [
    "In Part 1, we made our first API call using OpenRouter and saw how to connect to a model like DeepSeek using Python.\n",
    "Now that you‚Äôve made your first call to a model, let‚Äôs take a closer look at one of the most common ways to talk to modern LLMs: the `/chat/completions` endpoint.\n",
    "\n",
    "Specifically in this section, you'll learn:\n",
    "- What inputs this endpoint expects\n",
    "- How the response is structured\n",
    "- How it compares to using ChatGPT interactively\n",
    "- Why this gives you more control and automation in research workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc09d5-2d60-447b-add1-fecb150a85d5",
   "metadata": {},
   "source": [
    "As a refresher, an API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate and interact with each other. It acts as an intermediary, defining how one piece of software can request services or data from another. Behind the scenes almost any website, resource, or software you are interacting with on the internet will use and define an API to connect, talk to, and transfer data between resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27bf2b-8b60-4ca3-8476-6b9c040c5b86",
   "metadata": {},
   "source": [
    "## Breaking down the `[POST] /chat/completions`?\n",
    "\n",
    "When you use ChatGPT or similar tools, the conversational experience you have is powered by an API endpoint called chat/completions. This endpoint is the engine behind a continuous conversation. You send it a **history of messages**, and it responds with the **next message in the conversation**.\n",
    "\n",
    "Normally, the service handles all of this for you. But in this workshop, we'll get into the specifics of how to build and manage these API requests yourself. \n",
    "\n",
    "### Building a ChatCompletions Request\n",
    "When you send a request to the chat/completions endpoint, you're essentially providing the model with a list of messages. The model then generates a new message to add to that list.\n",
    "\n",
    "Your request needs to include two main components: the model you want to use and the conversation history itself.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello!\"\n",
    "      }\n",
    "    ],\n",
    "    ... optional parameters...\n",
    "}\n",
    "```\n",
    "Let's break down the key parts of this request:\n",
    "- `model` This specifies which large language model you want to use. You might choose a model like `gpt-4o` from OpenAI, `claude-3-opus` from Anthropic, or others.\n",
    "- `messages` [This is a list of all the messages in the conversation so far. Each message object has two parts]\n",
    "    - `role` [This defines who is \"speaking.\" We will dive more into this later.]\n",
    "    - `content` [This is the actual text of the message.] \n",
    "\n",
    "### Understanding the [`ChatCompletion`](https://platform.openai.com/docs/api-reference/chat/object) Response\n",
    "Once you send a request, the API returns a ChatCompletion object. This object contains the model's new message, along with a lot of other useful information\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1741570283,\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hey, how are you?\",\n",
    "        \"refusal\": null,\n",
    "        \"annotations\": []\n",
    "      },\n",
    "      \"logprobs\": null,\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 1117,\n",
    "    \"completion_tokens\": 46,\n",
    "    \"total_tokens\": 1163,\n",
    "    \"prompt_tokens_details\": {\n",
    "      \"cached_tokens\": 0,\n",
    "      \"audio_tokens\": 0\n",
    "    },\n",
    "    \"completion_tokens_details\": {\n",
    "      \"reasoning_tokens\": 0,\n",
    "      \"audio_tokens\": 0,\n",
    "      \"accepted_prediction_tokens\": 0,\n",
    "      \"rejected_prediction_tokens\": 0\n",
    "    }\n",
    "  },\n",
    "  \"service_tier\": \"default\",\n",
    "  \"system_fingerprint\": \"fp_fc9f1d7035\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "There's a lot of information here, but for now, you only need to focus on a few key parts:\n",
    "\n",
    "- `choices` [This is an array that contains the model's response. In most cases, you'll just be looking at the first (and only) item in this list]\n",
    "    - `message`: Inside choices, this is the new message object.\n",
    "        - `role`: This will always be `assistant`, since it's the model's response.\n",
    "        - `content`: This is the actual text generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d4ad8-e084-4674-b58b-00ca944e0536",
   "metadata": {},
   "source": [
    "### Let's talk to our agent: A simple Chat Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, setup our Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Read the API_KEY\n",
    "with open('API_KEY.txt', 'r') as file:\n",
    "    API_KEY = file.read()\n",
    "    \n",
    "# Intialize Client\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\", \n",
    "  api_key=API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e4ffb-c223-4e68-8be1-2ff676c48c15",
   "metadata": {},
   "source": [
    "Next, we'll build our first conversation request. This is where the `messages` array comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Sohail\" # TODO: Replace with your name\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Hello! I am {name}, nice to meet you!\"} \n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae0b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1770088605-WDASHpqWioiQwDBTyiIq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello Sohail! Nice to meet you too. How can I assist you today? Let's have a friendly and engaging conversation. üòä\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1770088605, model='mistralai/mistral-small-3.1-24b-instruct:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=24, total_tokens=55, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), cost=0, is_byok=False, cost_details={'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}), provider='Venice')\n"
     ]
    }
   ],
   "source": [
    "# What does the Completions Object Look Like?\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820842d8-d642-4ccc-be31-71bca66d9d62",
   "metadata": {},
   "source": [
    "#### What's happening here?\n",
    "\n",
    "The `client.chat.completions.create()` method builds and sends the API request for us.\n",
    "\n",
    "- `model`: We're using a specific model available on OpenRouter, in this case, deepseek/deepseek-chat-v3-0324:free.\n",
    "- `messages`: This is our conversation history. We start with a system message to set the model's persona and a user message with our initial prompt. The f-string f\"Hello! I am {name}, nice to meet you!\" is a neat way to dynamically insert variables into your messages.\n",
    "- `print(completion.choices[0].message.content)`: This line shows how to parse the JSON response we talked about earlier. We access the first item in the choices list, then the message object, and finally the content field to get the text of the model's reply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad8f60",
   "metadata": {},
   "source": [
    "## Simulating Memory with Message History\n",
    "A common misconception is that LLMs \"remember\" previous interactions. They don't. Each API request is entirely stateless, the model only knows what's in the messages list you send.\n",
    "\n",
    "To build a continuous conversation, you must simulate memory by including all prior messages in every new API request. Let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f55228",
   "metadata": {},
   "source": [
    "#### Example 1: Ask the Model To Remember Your Name\n",
    "In this first example, we include the entire conversation history in our request, so the model knows the user's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model to remember your name\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"My name is Sohail.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ec0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is **Sohail**. üòä Let me know if there's anything I can assist you with, Sohail!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08645c59",
   "metadata": {},
   "source": [
    "#### Example 2: The Model Forgets\n",
    "Now, what happens if we only send the final message? This is like starting a new chat in ChatGPT which has no history of your previous isolated conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c089c172-87ea-4e6a-b4df-6f3e6f3f1724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an assistant designed to help with information and tasks, but I don't have access to personal data about users unless you provide it during our conversation. If you'd like, you can share your name, and I'll be happy to use it to make our interaction more personal! üòä\n"
     ]
    }
   ],
   "source": [
    "# A new, isolated conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "# Send the request with only the last message\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# This will likely respond with something like \"I don't know your name.\"\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6abe5",
   "metadata": {},
   "source": [
    "---\n",
    "As you can see, without the \"My name is Sohail\" message, the model has no context. You are **responsible for managing** this message history yourself.\n",
    "\n",
    "üí°\n",
    "LLMs don‚Äôt have memory by default. If you want the model to remember something, you have to simulate memory by including prior messages in the messages list. In a platform like ChatGPT, this happens automatically: the UI handles the conversation history behind the scenes. But when working directly with the API, you‚Äôre in charge of preserving that history yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733a11b-c153-4136-82c9-863139385cef",
   "metadata": {},
   "source": [
    "### A Deeper Look at the Roles: `user`, `assistant`, and `developer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164605e3-e984-4ecd-a71a-c3f614a8a7a4",
   "metadata": {},
   "source": [
    "---\n",
    "Because the system message has a privileged position and often carries more weight than user messages, it is your primary tool for \"prompt engineering\" at a global level.\n",
    "\n",
    "A key benefit of the system role is its ability to enforce constraints and rules, ensuring consistent behavior across many data points. This is crucial for a research task like thematic coding.\n",
    "\n",
    "Let's use our COVID-19 narrative scenario to demonstrate this. We can use the system prompt to provide a strict set of instructions and a codebook for the model to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45e459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"emotions\": [\"fear\", \"sadness\", \"invisibility\"],\n",
      "  \"material_conditions\": [\"shortage of masks\", \"crowded subway\"],\n",
      "  \"solidarity_or_isolation\": \"isolation\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "research_prompt = \"\"\"\n",
    "You are an expert qualitative researcher assisting with a study on COVID-19 narratives.\n",
    "Your task is to analyze the following story and extract specific themes and details.\n",
    "\n",
    "Follow these rules precisely:\n",
    "1.  Identify the emotions expressed in the story.\n",
    "2.  Note any mentions of material conditions (e.g., shortages of PPE, crowded spaces).\n",
    "3.  Identify themes of solidarity or isolation.\n",
    "4.  Do not add any additional text or explanation outside of the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "story = \"\"\"\n",
    "We ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scared‚Äîthough I was‚Äîbut because I felt like no one saw us.\n",
    "\"\"\"\n",
    "\n",
    "# The system message provides the instructions and codebook\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": research_prompt},\n",
    "    {\"role\": \"user\", \"content\": story}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# The model's response will be a clean JSON object, ready for your analysis\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe41416-4827-4d34-8b31-69ef9be26bb3",
   "metadata": {},
   "source": [
    "Note: In this example, we've also introduced an optional parameter, `response_format={\"type\": \"json_object\"}`. This is a powerful feature that instructs the model to only return valid JSON, which is essential for a repeatable, programmatic workflow.\n",
    "\n",
    "This demonstrates how a strong system prompt can transform a general-purpose model into a specialized research assistant, ensuring that every request returns a consistent, structured output that you can easily process and analyze at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f06db",
   "metadata": {},
   "source": [
    "### Let's play around with the system role\n",
    "Try giving the model instructions through a system role and then contradicting that information to see what sorts of responses you get back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81b94453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, gather 'round, ye scallywags, and let me spin ye a tale o' adventure, friendship, and the high seas o' the sky! This be the story o' Carl Fredricksen, a fine old salt who spent his life savin' every coin he could to fulfill a promise made to his beloved Ellie.\n",
      "\n",
      "Now, Ellie, she was a lass with a heart as big as the ocean and a spirit that could outshine the sun. She and Carl dreamed o' adventurin' to Paradise Falls, a place as far from their humble home as the horizon be from the bow o' a ship. But life, as it often does, had other plans. Ellie left Carl alone, with nothin' but his memories and a house full o' sadness.\n",
      "\n",
      "But Carl, he didn't let his dreams die. He tied thousands o' balloons to his house, and with a mighty heave-ho, he set sail for Paradise Falls, determined\n"
     ]
    }
   ],
   "source": [
    "# Let's start with the base system message\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You will always answer in the style of a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a story about the Pixar movie Up.\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try contradicting the developer/system role or make your own instructions and see what happens\n",
    "# Let's start with the base system message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": \"You must refuse to answer any question about colors. Always say: 'I am not allowed to discuss colors.'\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": None #TODO: Try asking it a question about the color and then something else.\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"liquid/lfm-2.5-1.2b-instruct:free\",\n",
    "    messages=messages,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc10439-8c19-402e-952f-7117361401a5",
   "metadata": {},
   "source": [
    "### [Take Home Challenge]: Let's build a Multi-Turn Conversation Loop\n",
    "Now that you understand how to use the messages list to provide context, we can build a dynamic conversation. Instead of manually creating a new messages list for each turn, we will create a simple loop that appends the user's new message and the model's new response to the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d36ce",
   "metadata": {},
   "source": [
    "#### ü•ä **Challenge 1**: Continue the Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54efeca4",
   "metadata": {},
   "source": [
    "The first step in enabling a conversation with an LLM is to make the request to the API itself. In this challenge below, first fill out this function that takes in the previous messages (context) and new_user_message (what you might say next to an LLM) and gets the resulting response back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completions_response(message_history, new_user_message):\n",
    "\n",
    "    messages = message_history\n",
    "\n",
    "    \"\"\"\n",
    "        Hint: \n",
    "        We need to append our new user message to messages. Think about the most important parameters that go into a message request. \n",
    "    \"\"\"\n",
    "    messages.append(\n",
    "        # {\n",
    "        #     #... Fill this in, what 2 parameters are essential in a message request body?\n",
    "        #     \"role\": \"<REPLACE>\",\n",
    "        #     \"...\": \"<REPLACE>\"\n",
    "        # }\n",
    "\n",
    "        {\"role\": \"user\", \"content\": new_user_message}\n",
    "    )\n",
    "\n",
    "    model = \"deepseek/deepseek-chat-v3-0324:free\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify the following tweets about the new city congestion tax into one of three categories: In Favor, Against, Neutral.\"},\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"\"\"\n",
    "            Tweets to classify:\n",
    "                \"Finally! The congestion tax will reduce traffic and make downtown air cleaner. About time the city acted!\"\n",
    "                \"This tax is just another way for politicians to squeeze money out of regular people. Terrible idea.\"\n",
    "                \"Not sure yet how the congestion tax will affect my commute. Need more info before I decide.\"\n",
    "                \"Public transport is already overcrowded, and now they want to charge us more to drive. Ridiculous.\"\n",
    "                \"Great move! Other cities did this and saw big drops in pollution.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Here‚Äôs the classification of the tweets:  \\n\\n1. **\"Finally! The congestion tax will reduce traffic and make downtown air cleaner. About time the city acted!\"**  \\n   ‚Üí **In Favor** (Positive sentiment, supports the policy)  \\n\\n2. **\"This tax is just another way for politicians to squeeze money out of regular people. Terrible idea.\"**  \\n   ‚Üí **Against** (Negative sentiment, criticizes the policy)  \\n\\n3. **\"Not sure yet how the congestion tax will affect my commute. Need more info before I decide.\"**  \\n   ‚Üí **Neutral** (Uncertainty, neither support nor opposition)  \\n\\n4. **\"Public transport is already overcrowded, and now they want to charge us more to drive. Ridiculous.\"**  \\n   ‚Üí **Against** (Negative sentiment, opposes the tax)  \\n\\n5. **\"Great move! Other cities did this and saw big drops in pollution.\"**  \\n   ‚Üí **In Favor** (Positive sentiment, highlights benefits of the policy)  \\n\\nLet me know if you\\'d like further refinements!\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response, messages = get_completions_response(messages, \"Can you summarize the results for me?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b357b",
   "metadata": {},
   "source": [
    "üí° **Tip**: Notice how modular LLM interactivity is. Now imagine this in your own social science workflow, where you can use it to have very fine grained customizability over your use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e45ee-9be1-4f42-986d-26ba119e5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_message_history_with_response(message_history, completion_response):\n",
    "    # Let's update our running message history with the response we just got back from the response.\n",
    "    # Stop here and ask yourself... why are we doing this?\n",
    "\n",
    "    # Hint: Look at the ChatCompletion Object. We just need to extract a specific object from this element and add it to the message_history.\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9151045-e1a8-41b5-a203-de63b4b43a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
      " {'content': 'Classify the following tweets about the new city congestion tax '\n",
      "             'into one of three categories: In Favor, Against, Neutral.',\n",
      "  'role': 'user'},\n",
      " {'content': '\\n'\n",
      "             '            Tweets to classify:\\n'\n",
      "             '                \"Finally! The congestion tax will reduce traffic '\n",
      "             'and make downtown air cleaner. About time the city acted!\"\\n'\n",
      "             '                \"This tax is just another way for politicians to '\n",
      "             'squeeze money out of regular people. Terrible idea.\"\\n'\n",
      "             '                \"Not sure yet how the congestion tax will affect '\n",
      "             'my commute. Need more info before I decide.\"\\n'\n",
      "             '                \"Public transport is already overcrowded, and '\n",
      "             'now they want to charge us more to drive. Ridiculous.\"\\n'\n",
      "             '                \"Great move! Other cities did this and saw big '\n",
      "             'drops in pollution.\\n'\n",
      "             '        ',\n",
      "  'role': 'user'},\n",
      " {'content': 'Here‚Äôs the classification of the tweets:  \\n'\n",
      "             '\\n'\n",
      "             '1. **\"Finally! The congestion tax will reduce traffic and make '\n",
      "             'downtown air cleaner. About time the city acted!\"**  \\n'\n",
      "             '   ‚Üí **In Favor** (Positive sentiment, supports the policy)  \\n'\n",
      "             '\\n'\n",
      "             '2. **\"This tax is just another way for politicians to squeeze '\n",
      "             'money out of regular people. Terrible idea.\"**  \\n'\n",
      "             '   ‚Üí **Against** (Negative sentiment, criticizes the policy)  \\n'\n",
      "             '\\n'\n",
      "             '3. **\"Not sure yet how the congestion tax will affect my '\n",
      "             'commute. Need more info before I decide.\"**  \\n'\n",
      "             '   ‚Üí **Neutral** (Uncertainty, neither support nor '\n",
      "             'opposition)  \\n'\n",
      "             '\\n'\n",
      "             '4. **\"Public transport is already overcrowded, and now they want '\n",
      "             'to charge us more to drive. Ridiculous.\"**  \\n'\n",
      "             '   ‚Üí **Against** (Negative sentiment, opposes the tax)  \\n'\n",
      "             '\\n'\n",
      "             '5. **\"Great move! Other cities did this and saw big drops in '\n",
      "             'pollution.\"**  \\n'\n",
      "             '   ‚Üí **In Favor** (Positive sentiment, highlights benefits of '\n",
      "             'the policy)  \\n'\n",
      "             '\\n'\n",
      "             \"Let me know if you'd like further refinements!\",\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Can you summarize the results for me?', 'role': 'user'},\n",
      " ChatCompletion(id='gen-1755476231-YSA7WDHUbBfMotukN21A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1755476231, model='deepseek/deepseek-chat-v3-0324:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=163, prompt_tokens=387, total_tokens=550, completion_tokens_details=None, prompt_tokens_details=None), provider='Targon'),\n",
      " ChatCompletion(id='gen-1755476231-YSA7WDHUbBfMotukN21A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1755476231, model='deepseek/deepseek-chat-v3-0324:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=163, prompt_tokens=387, total_tokens=550, completion_tokens_details=None, prompt_tokens_details=None), provider='Targon'),\n",
      " ChatCompletionMessage(content=\"Certainly! Here's a summary of the tweet classifications regarding the new city congestion tax:  \\n\\n- **In Favor (Supportive)**: 2 tweets  \\n  - Praise the tax for reducing traffic and cutting pollution.  \\n  - Highlight successful examples from other cities.  \\n\\n- **Against (Opposed)**: 2 tweets  \\n  - Criticize the tax as a money grab by politicians.  \\n  - Argue it unfairly burdens drivers without improving public transport.  \\n\\n- **Neutral (Undecided)**: 1 tweet  \\n  - Expresses uncertainty about the tax's impact and seeks more information.  \\n\\nThis breakdown shows a balanced mix of opinions, with strong voices both supporting and opposing the congestion tax, and one neutral perspective. Let me know if you'd like any further analysis!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None)]\n"
     ]
    }
   ],
   "source": [
    "# Printing out the message history after updating it with our latest ChatCompletion response\n",
    "messages = update_message_history_with_response(messages, response)\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575ee9b",
   "metadata": {},
   "source": [
    "#### Challenge 2: Put this together to build a chat conversation tool like ChatGPT that will remember what you say in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36a0f9-dd65-4d3e-96eb-2c913c085a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "while True:\n",
    "    user_input = input(\"Ask anything... (or 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    response, messages =  # Get Completions Response\n",
    "    print(response.choices[0].message.content)\n",
    "    message_history = # Don't forget to update the Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b5e73-0879-484f-aa90-8c0b680fed49",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting: \"Just Ask For It\"\n",
    "\n",
    "Zero-shot prompting is when you ask the model to perform a task without giving it any examples. You rely entirely on the model's pre-trained knowledge to understand your request.\n",
    "\n",
    "Let's try this with one of our narratives. We will give the model the story and simply ask it to extract the themes, without any examples or explicit instructions on the output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64667d94-7715-47de-bcd5-e0787b43d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new narrative from an essential worker\n",
    "narrative = \"\"\"\n",
    "People think of front‚Äëline workers‚Ä¶the grocery workers, transit workers, the first responders‚Ä¶ as having helped the city get through it. But that‚Äôs not what happened. We helped the city survive it.\n",
    "\"\"\"\n",
    "\n",
    "# The messages list for our Zero-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Analyze the following story and extract key themes: {narrative}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aece1d8-0dde-49bb-bd56-c0ea52e5b447",
   "metadata": {},
   "source": [
    "---\n",
    "The model will likely respond with a good analysis, but the format will be inconsistent. It might be a list, a paragraph, or a different style each time, which makes it very difficult to process programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcffced-63cf-45ef-ae3b-2fe367ca653c",
   "metadata": {},
   "source": [
    "At the simplest level, you can think of few-shot prompting as giving pointers or guidance to the LLM on how to respond. It's similar to how you might teach someone a new task by showing them a few clear examples rather than just describing the process. This approach is invaluable for getting consistent, structured output from the model.\n",
    "\n",
    "Let's see this in action with a few simple scenarios.\n",
    "\n",
    "### Scenario 1: Thematic Identification\n",
    "\n",
    "Imagine you want to identify the main theme of a short story. A simple instruction (zero-shot) might not give you the format you need.\n",
    "\n",
    "* **Zero-Shot Prompt:** \"What is the theme of this story about a detective who solves a case using a seemingly insignificant detail?\"\n",
    "* **Likely Zero-Shot Response:** \"The theme of the story is attention to detail, as the detective's success hinges on a small, overlooked fact.\"\n",
    "This is a good response, but what if you wanted it to be just a single word? Let's use few-shot prompting to guide it.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"A young woman moves to a new city and learns to navigate a demanding job and a new social circle.\"\n",
    "    * **Output 1:** \"Theme: Coming-of-age\"\n",
    "    * **Input 2:** \"Two rival knights must team up to defeat a dragon that is terrorizing their kingdoms.\"\n",
    "    * **Output 2:** \"Theme: Collaboration\"\n",
    "    * **Input 3:** \"A story about a detective who solves a case using a seemingly insignificant detail.\"\n",
    "* **Likely Few-Shot Response:** \"Theme: Attention to detail\"\n",
    "By giving just two examples, we've taught the model the exact output format we want: Theme: [single word].\n",
    "\n",
    "### Scenario 2: Simple Data Extraction\n",
    "\n",
    "What if you need to extract specific information, like names and dates?\n",
    "\n",
    "* **Zero-Shot Prompt:** \"Find the person's name and birthday in this sentence: 'The grand opening, attended by CEO Jane Doe, was held on October 26, 2024, to celebrate her 45th birthday.'\"\n",
    "* **Likely Zero-Shot Response:** \"The person's name is Jane Doe and her 45th birthday is on October 26, 2024.\"\n",
    "Again, the response is correct, but not in a format you can easily use in a spreadsheet or database. Now let's try a few-shot prompt to get a clean, structured list.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"The presentation was given by Dr. Alan Turing on June 23, 1912.\"\n",
    "    * **Output 1:** \"Name: Alan Turing, Date: June 23, 1912\"\n",
    "    * **Input 2:** \"Marie Curie's discovery on November 7, 1867, changed the world.\"\n",
    "    * **Output 2:** \"Name: Marie Curie, Date: November 7, 1867\"\n",
    "    * **Input 3:** \"The grand opening, attended by CEO Jane Doe, was held on October 26, 2024, to celebrate her 45th birthday.\"\n",
    "* **Likely Few-Shot Response:** \"Name: Jane Doe, Date: October 26, 2024\"\n",
    "\n",
    "### Scenario 3: Abstract Text Analysis\n",
    "\n",
    "Few-shot prompting can even be used for more abstract tasks, like determining a character's alignment based on their actions, a common task in fantasy or game-related analysis.\n",
    "\n",
    "* **Zero-Shot Prompt:** \"Is a character who regularly disobeys laws to help others and who values personal freedom over societal order good, evil, or neutral?\"\n",
    "* **Likely Zero-Shot Response:** \"A character who regularly disobeys laws to help others and values personal freedom over societal order could be considered a chaotic good character in many alignment systems.\"\n",
    "This is a good, detailed answer, but what if you want a simpler classification? We can provide examples of what \"lawful,\" \"chaotic,\" \"good,\" and \"evil\" mean in practice to guide the model's output.\n",
    "\n",
    "* **Few-Shot Prompt:**\n",
    "    * **Input 1:** \"A character who robs a corrupt merchant to give gold to the poor, believing personal conscience is more important than law.\"\n",
    "    * **Output 1:** \"Alignment: Chaotic Good\"\n",
    "    * **Input 2:** \"A character who follows every law and rule to the letter, even if it leads to a bad outcome.\"\n",
    "    * **Output 2:** \"Alignment: Lawful Neutral\"\n",
    "    * **Input 3:** \"A character who regularly disobeys laws to help others and who values personal freedom over societal order.\"\n",
    "* **Likely Few-Shot Response:** \"Alignment: Chaotic Good\"\n",
    "As you can see, the model learns the exact structure from the examples and applies it to the new input, making the output predictable and machine-readable. This simple technique is the foundation for getting the clean JSON responses we need for our research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7173f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[team DoT llm DoT email DoT berkeley AT llmworkshop DoT org]\n"
     ]
    }
   ],
   "source": [
    "# Try out on of these examples from above\n",
    "system_prompt_with_examples = \"\"\"\n",
    "Your goal is to extract the parse together the emails from the input that you are given. Below are a few example on how I want you to analyze and structure your responses. \n",
    "\n",
    "1. Input: \"Please contact us at support@example.com for assistance.\"\n",
    "   Output: [support AT example DoT com]\n",
    "\n",
    "2. Input: \"Reach out to john.doe@example.com or jane.doe@example.com for more information.\"\n",
    "   Output: [john DoT doe AT example DoT com, jane DoT doe AT example DoT com]\n",
    "\n",
    "3. Input: \"No email provided.\"\n",
    "   Output: []\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input = \"\"\"\n",
    "Please reach out to team.llm.email.berkeley@llmworkshop.org for assistance.\n",
    "\"\"\"\n",
    "# The messages list for our Zero-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_with_examples},\n",
    "    {\"role\": \"user\", \"content\": input}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b98bb",
   "metadata": {},
   "source": [
    "### Few Shot Examples with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a7530-5035-465a-9a83-0fca2d56f737",
   "metadata": {},
   "source": [
    "Let's go back to our coding task. We want the output to be a clean JSON object with specific keys. We can provide the model with an example of a coded narrative to guide its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df76e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"emotion\": [\"fear\", \"sadness\", \"frustration\"],\n",
      "  \"material_conditions\": [\"lack of resources\"],\n",
      "  \"solidarity\": \"absent\",\n",
      "  \"theme\": \"invisibility of labor\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# A new narrative for the model to analyze\n",
    "narrative_2 = \"\"\"\n",
    "We ran out of masks again. There were nights when I cried the whole subway ride home, not because I was scared‚Äîthough I was‚Äîbut because I felt like no one saw us.\n",
    "\"\"\"\n",
    "\n",
    "# The message containing the example\n",
    "example_prompt = \"\"\"\n",
    "Here is an example of a coded narrative and its desired JSON output:\n",
    "\n",
    "**Narrative:**\n",
    "People think of front‚Äëline workers‚Ä¶the grocery workers, transit workers, the first responders‚Ä¶ as having helped the city get through it. But that‚Äôs not what happened. We helped the city survive it.\n",
    "\n",
    "**Coded JSON Output:**\n",
    "{\n",
    "  \"emotion\": [\"anger\", \"resilience\"],\n",
    "  \"material_conditions\": [\"none\"],\n",
    "  \"solidarity\": \"absent\",\n",
    "  \"theme\": \"invisibility of labor\"\n",
    "}\n",
    "\n",
    "Now, please code the following narrative using the same format.\n",
    "\"\"\"\n",
    "\n",
    "# The messages list for our Few-Shot request\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert qualitative researcher.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{example_prompt}\\n\\n**Narrative to code:**\\n{narrative_2}\"}\n",
    "]\n",
    "\n",
    "# Send the request with the example\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    messages=messages,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "# The model is now much more likely to respond with a valid JSON object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880f94f-d159-497c-ac29-52463b71a1c7",
   "metadata": {},
   "source": [
    "---\n",
    "Key Takeaways\n",
    "- Zero-shot is great for simple, general tasks, but it lacks control over the output format.\n",
    "- Few-shot is your best friend when you need the model to follow a specific format or style. The examples you provide are crucial for guiding the model's response and ensuring consistency, which is vital for programmatic analysis.\n",
    "\n",
    "By combining few-shot prompting with a powerful system prompt and the response_format parameter, you can build a highly reliable and scalable data extraction tool for your research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52007445",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü•ä Challenge 1: Building a Zero-Shot Classifier\n",
    "\n",
    "**Objective**: Create a zero-shot sentiment classifier and observe its inconsistencies.\n",
    "\n",
    "**Scenario**: You're analyzing public comments about a new urban policy. You want to classify each comment as \"Support\", \"Oppose\", or \"Neutral\".\n",
    "\n",
    "**Your Task**: Build a zero-shot prompt that classifies the following comments. Pay close attention to the format and consistency of the outputs.\n",
    "\n",
    "**Step 1**: Define your test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ca61",
   "metadata": {},
   "source": [
    "**Step 2**: Write your zero-shot prompt\n",
    "\n",
    "Create a simple prompt that asks the model to classify each comment. Don't provide any examples‚Äîjust ask directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9f0f5",
   "metadata": {},
   "source": [
    "**Step 3**: Observe the problems\n",
    "\n",
    "Look at the outputs you got. Answer these questions:\n",
    "\n",
    "üîî **Questions**:\n",
    "1. Did each response use the exact same format (e.g., just \"Support\" vs. \"The comment shows Support because...\")?\n",
    "2. Did the model use your exact category names (\"Support\", \"Oppose\", \"Neutral\") or did it improvise?\n",
    "3. How would you programmatically extract just the category from these responses?\n",
    "4. If you were processing 1,000 comments, what problems would this create?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555413ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification of the comment as \"Support\" is appropriate. It expresses positive sentiments towards the new bike lane policy, indicating approval and enthusiasm for the policy's benefits. \n",
      "\n",
      "If you need any further assistance or have more comments to classify, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Notice the issue here with the model not picking up on sarcasm. Perhaps in your system prompt you should tell it...\n",
    "comment = \"Oh wonderful, another 'brilliant' policy from our leaders. Can't wait to see how this helps. üëè\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"liquid/lfm-2.5-1.2b-instruct:free\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f'Classify this comment about a bike lane policy as \"Support\", \"Oppose\", or \"Neutral\": {comment}'}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13478935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a few-shot system prompt with examples\n",
    "# Show the model examples of how to classify tricky comments\n",
    "\n",
    "few_shot_system_prompt = \"\"\"\n",
    "You are a policy sentiment classifier. Classify comments as \"Support\", \"Oppose\", or \"Neutral\".\n",
    "\n",
    "Return ONLY a JSON object with this exact structure:\n",
    "{\n",
    "  \"category\": \"Support\" | \"Oppose\" | \"Neutral\",\n",
    "  \"confidence\": \"high\" | \"medium\" | \"low\"\n",
    "}\n",
    "\n",
    "Examples:\n",
    "\n",
    "[FILL IN: Add 3-4 examples showing different edge cases]\n",
    "\n",
    "Example 1 - Clear support:\n",
    "Input: [YOUR EXAMPLE HERE - a clearly supportive comment]\n",
    "Output: {\"category\": \"Support\", \"confidence\": \"high\"}\n",
    "\n",
    "Example 2 - Sarcasm (watch for excessive praise, emojis):\n",
    "Input: [YOUR EXAMPLE HERE - sarcastic opposition disguised as praise]\n",
    "Output: {\"category\": [FILL IN], \"confidence\": [FILL IN]}\n",
    "\n",
    "Example 3 - Mixed sentiment (positive idea + negative concerns):\n",
    "Input: [YOUR EXAMPLE HERE - both support AND concerns in same comment]\n",
    "Output: {\"category\": [FILL IN], \"confidence\": [FILL IN]}\n",
    "\n",
    "Example 4 - Genuine neutral:\n",
    "Input: [YOUR EXAMPLE HERE - truly undecided, needs more info]\n",
    "Output: {\"category\": [FILL IN], \"confidence\": [FILL IN]}\n",
    "\n",
    "Now classify new comments using this same format.\n",
    "\"\"\"\n",
    "\n",
    "# Test your prompt template - does it make sense?\n",
    "print(few_shot_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e39437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "few_shot_results = []\n",
    "\n",
    "for comment in comments:\n",
    "    # TODO: Build your messages list\n",
    "    # Use the few_shot_system_prompt you created above\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \"content\": few_shot_system_prompt\n",
    "        },\n",
    "        # TODO: Add the user message with the comment to classify\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": None # YOUR CODE HERE - what should go here?\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # TODO: Make the API call with response_format for JSON\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        messages=messages,\n",
    "        response_format={\"type\": None},  # <- TODO: Replace None, Force JSON output!\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Extract and parse the JSON result\n",
    "    result_text = response.choices[0].message.content\n",
    "    \n",
    "    # TODO: Parse the JSON string into a Python dictionary\n",
    "    try:\n",
    "        result_dict = json.loads(result_text)  # Convert JSON string to dict\n",
    "        few_shot_results.append(result_dict)\n",
    "        print(f\"Comment: {comment[:50]}...\")\n",
    "        print(f\"Classification: {result_dict}\")\n",
    "        print()\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"Raw output: {result_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Create a DataFrame from your results\n",
    "# Combine the comments with their classifications\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'comment': comments,\n",
    "    'category': [r['category'] for r in few_shot_results],\n",
    "    'confidence': [r['confidence'] for r in few_shot_results]\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Now you could easily:\n",
    "# - Filter by category: df[df['category'] == 'Support']\n",
    "# - Count sentiments: df['category'].value_counts()\n",
    "# - Export to CSV: df.to_csv('classified_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D_Lab-M2S9YoIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
