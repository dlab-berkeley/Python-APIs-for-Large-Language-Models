{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99ebdde",
   "metadata": {},
   "source": [
    "# Part 4: Tool Calling - Giving Your LLM Research Superpowers\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<h3>🎯 Workshop Goals (Part 4)</h3>\n",
    "<p>In this final hour, we will give our LLM the ability to interact with the outside world. This is the key to unlocking its potential as a true research assistant. You will learn:</p>\n",
    "<ul>\n",
    "<li><strong>Why a Researcher Needs Tools:</strong> Understand the inherent limitations of LLMs, such as knowledge cutoffs and the inability to access your private data or the live internet.</li>\n",
    "<li><strong>The Tool Calling Loop:</strong> Master the fundamental request-execute-respond pattern that powers all modern AI agents.</li>\n",
    "<li><strong>Building a Research Assistant:</strong> Create a set of tools that could help automate a social science research workflow.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9eee4",
   "metadata": {},
   "source": [
    "1. The Researcher's Dilemma: The Brain in the Jar\n",
    "So far, we've treated the LLM as an incredibly knowledgeable, fast, and reliable data processor. You give it text, and it gives you back perfectly structured data. This is a powerful skill for tasks like the COVID-19 narrative analysis we've discussed.\n",
    "\n",
    "But for a researcher, this is only half the battle. Your work isn't static; it's a dynamic process. This exposes two major limitations of a standard LLM:\n",
    "\n",
    "1. **The Knowledge Cutoff**: The model's knowledge stops the moment its training was completed. It cannot read the latest paper published on arXiv yesterday, access the most recent census data, or know about a news event that happened this morning. Its knowledge is a fixed library, not a live internet connection.\n",
    "2. **The Inability to Take Action**: The LLM is isolated. It can't access your local files, query your university's research database, or even run a simple statistical calculation. It can write a script to analyze your data, but it can't run it.\n",
    "\n",
    "Imagine you have a brilliant research assistant who is locked in the university's library archives from two years ago. They can read and synthesize any book in that room with superhuman speed, but they can't use a web browser, make a phone call, or open an Excel spreadsheet on your computer.\n",
    "\n",
    "Tool calling is the act of giving that brilliant assistant a laptop and an internet connection. It connects the \"brain\" of the LLM to the \"hands\" of real-world tools, allowing it to overcome these limitations and become a true partner in your research workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726ec5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### How it Works: The Tool Calling Loop\n",
    "At its core, tool calling is a multi-step conversation between your code and the LLM. It's not a single request and response, but a sequence of exchanges. We call this the Tool Calling Loop.\n",
    "\n",
    "Let's make this concrete with a simple, non-research example: asking for the current time. The LLM doesn't have a live clock, so it will need a tool for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e3e2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 1: Define Your Tool in Python\n",
    "First, we write a standard Python function that does the thing we need. This function is our \"tool.\" It's just regular code that runs on your machine, not in the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134bd239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need our client and some other libraries\n",
    "import json\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "# Read the API_KEY\n",
    "with open('API_KEY.txt', 'r') as file:\n",
    "    API_KEY = file.read() \n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf396aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  TOOL EXECUTED: get_current_time() ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2025-10-08 23:45:14'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our tool. A simple Python function to get the current time.\n",
    "def get_current_time():\n",
    "    \"\"\"Gets the current local time.\"\"\"\n",
    "    # In a real application, this could be doing anything:\n",
    "    # querying a database, calling another API, reading a file, etc.\n",
    "    print(\"---  TOOL EXECUTED: get_current_time() ---\")\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "get_current_time()  # Test the tool works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0d6a0",
   "metadata": {},
   "source": [
    "#### Step 2: Describe the Tool to the LLM\n",
    "\n",
    "Next, we need to create a \"manual\" or \"menu\" of our available tools for the LLM. We define this using a **specific JSON structure**. The description is the most important part! It's how the LLM decides when to use your tool. It should be clear and descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bca9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the \"menu\" of tools we will offer the LLM.\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_time\",\n",
    "            \"description\": \"Use this function to get the current date and time.\",\n",
    "            \"parameters\": { # This tool takes no parameters\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb41c3",
   "metadata": {},
   "source": [
    "#### Step 3: The First API Call - The LLM Asks for a Tool\n",
    "\n",
    "Now, we make our first API call. This is where the magic happens. The structure of the call is very similar to what you've seen before, but with two new, crucial parameters: `tools` and `tool_choice`.\n",
    "\n",
    "- `tools`: This is where we pass the \"menu\" of available tools that we defined in the previous step. It's a list of JSON objects describing each function the model is allowed to request.\n",
    "- `tool_choice`: This parameter controls how the model uses the tools.\n",
    "    - \"auto\" (the default): The model decides for itself whether to use a tool or not based on the user's prompt. This is what you'll use most of the time.\n",
    "    - \"none\": This forces the model to not use any tools and just respond with a standard message.\n",
    "    - {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}: This forces the model to use a specific function.\n",
    "\n",
    "This `tools` parameter is a core feature of the OpenAI API, and its standardization has been adopted by many other providers, including OpenRouter, making it a key skill to learn. You can read the complete, official documentation on this feature for more details.\n",
    "\n",
    "Official Documentation: [OpenAI API Reference - Tool Calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "\n",
    "Let's make the call. Notice the response we get back. The LLM doesn't answer the question directly. Instead, it asks us to run a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e52eb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user's question that requires a tool\n",
    "messages = [{\"role\": \"user\", \"content\": \"What time is it?\"}]\n",
    "\n",
    "# Make the first API call, providing the tools menu\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79b6af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': None,\n",
      " 'audio': None,\n",
      " 'content': '',\n",
      " 'function_call': None,\n",
      " 'reasoning': None,\n",
      " 'refusal': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{}', 'name': 'get_current_time'},\n",
      "                 'id': '94800ed9-2b25-4f6e-9e77-2fb69b67fe48',\n",
      "                 'index': 0,\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the response\n",
    "response_message = response.choices[0].message\n",
    "pprint(response_message.model_dump())  # Pretty-print the full response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba6e28",
   "metadata": {},
   "source": [
    "🔔 Question: Look at the response_message output. What is the content of the message? What is the value of the tool_calls field?\n",
    "\n",
    "You'll see that the message content is null. The LLM hasn't said anything. Instead, it has returned a `tool_calls` object. This is an instruction from the LLM to your code, saying: \"I need to answer the user, but first, please run the `get_current_time` function for me.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c34da",
   "metadata": {},
   "source": [
    "#### Step 4: Your Code Executes the Tool and Sends Back the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346886c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  TOOL EXECUTED: get_current_time() ---\n",
      "--- Current Message History ---\n",
      "{'content': 'What time is it?', 'role': 'user'}\n",
      "\n",
      "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='94800ed9-2b25-4f6e-9e77-2fb69b67fe48', function=Function(arguments='{}', name='get_current_time'), type='function', index=0)], reasoning=None)\n",
      "\n",
      "{'content': '2025-10-08 23:56:54',\n",
      " 'name': 'get_current_time',\n",
      " 'role': 'tool',\n",
      " 'tool_call_id': '94800ed9-2b25-4f6e-9e77-2fb69b67fe48'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, append the LLM's instruction to call a tool to our message history\n",
    "messages = [{\"role\": \"user\", \"content\": \"What time is it?\"}] # Our original user question\n",
    "messages.append(response_message)\n",
    "\n",
    "# Get the ID of the tool call\n",
    "tool_call_id = response_message.tool_calls[0].id\n",
    "\n",
    "# Call our actual Python function\n",
    "tool_output = get_current_time()\n",
    "\n",
    "# Now, append the *output* of our function to the message history\n",
    "# This tells the model what the result of its requested tool call was.\n",
    "messages.append(\n",
    "    {\n",
    "        \"tool_call_id\": tool_call_id,\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"content\": tool_output, # The actual result from our Python function\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"--- Current Message History ---\")\n",
    "for msg in messages:\n",
    "    pprint(msg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be92b11",
   "metadata": {},
   "source": [
    "#### Step 5: The Second API Call - The LLM Gives the Final Answer\n",
    "Now that the LLM has the real-time information it needed, we make one final API call. This time, the model uses the context from the tool's output to generate a natural, human-readable answer to the user's original question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ecd752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the second API call with the complete message history\n",
    "second_response = client.chat.completions.create(\n",
    "    model=\"alibaba/tongyi-deepresearch-30b-a3b:free\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35afebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Final Answer from LLM ---\n",
      "\n",
      "\n",
      "The current time is **23:56:54** on **2025-10-08**.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the final answer\n",
    "final_answer = second_response.choices[0].message.content\n",
    "print(\"\\\\n--- Final Answer from LLM ---\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87496c77",
   "metadata": {},
   "source": [
    "⚠️ **Warning:** Notice we've switched models here from `mistral-instruct` in the previous parts of the workshop to `tongyi-deepresearch-30b`. This is becuase some models don't support tool calling. On OpenRouter's website you can filter by models that support tool calling. Always ensure the model you're using can, otherwise this will not work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1087f",
   "metadata": {},
   "source": [
    "#### 🥊 [Challenge] Your First Research Tool: The Paper Fetcher (10 mins)\n",
    "Now it's your turn to build the entire tool-calling loop from scratch.\n",
    "\n",
    "**Your Goal**: Create a tool that allows the LLM to look up information about a (fake) academic paper based on its ID. This simulates a common research task: querying a database like arXiv, PubMed, or JSTOR.\n",
    "\n",
    "**Your Task**: You will need to write the code for all 5 steps of the loop to answer the user's question: \"Can you tell me the title and author of the paper 2305.15334?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05537905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Step 1: Define Your Tool in Python ---\n",
    "# This function simulates fetching data from an academic database.\n",
    "# It should take one argument: 'paper_id' (a string).\n",
    "def get_paper_details(paper_id: str):\n",
    "    \"\"\"\n",
    "    Gets the title and author for a given paper ID from a mock database.\n",
    "    \"\"\"\n",
    "    print(f\"--- TOOL EXECUTED: Searching for paper {paper_id} ---\")\n",
    "    \n",
    "    # A mock database of academic papers\n",
    "    mock_database = {\n",
    "        \"2305.15334\": {\n",
    "            \"title\": \"The Role of Social Media in Political Polarization\",\n",
    "            \"author\": \"Dr. Eleanor Vance\",\n",
    "            \"year\": 2023,\n",
    "        },\n",
    "        \"1706.03762\": {\n",
    "            \"title\": \"Attention Is All You Need\",\n",
    "            \"author\": \"Ashish Vaswani et al.\",\n",
    "            \"year\": 2017,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    paper_info = mock_database.get(paper_id, \"Paper not found.\")\n",
    "    return json.dumps(paper_info)\n",
    "\n",
    "\n",
    "# --- Step 2: Describe the Tool to the LLM ---\n",
    "# Create the 'tools' list with the definition for 'get_paper_details'.\n",
    "# Make sure you correctly define the 'parameters' the function expects!\n",
    "# It should have one required parameter: 'paper_id' of type 'string'.\n",
    "\n",
    "\n",
    "tool = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_paper_details\",\n",
    "            \"description\": \"Fetch details about a specific academic paper.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"paper_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The ID of the paper to retrieve.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"paper_id\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# --- Step 3, 4, and 5: Execute the Full Loop ---\n",
    "# Write the code to handle the entire conversation.\n",
    "\n",
    "# 1. Define the initial message list with the user's prompt.\n",
    "user_prompt = \"Can you tell me the title and author of the paper 2305.15334?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "# 2. Make the first API call to get the tool_calls object.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "    messages=messages,\n",
    "    tools=tool,\n",
    ")\n",
    "response_message = response.choices[0].message\n",
    "\n",
    "# 3. Check for tool_calls, execute the function, and append the results to the messages list.\n",
    "# (This part will be more complex than the time example, as you need to get the arguments!)\n",
    "\n",
    "if response_message.tool_calls:\n",
    "    # Extract the arguments for the tool call\n",
    "    tool_args = response_message.tool_calls[0].arguments\n",
    "    paper_id = tool_args.get(\"paper_id\")\n",
    "\n",
    "    # Call the function with the extracted arguments\n",
    "    tool_response = get_paper_details(paper_id)\n",
    "\n",
    "    # Append the tool response to the messages list\n",
    "    messages.append({\"role\": \"user\", \"content\": tool_response})\n",
    "\n",
    "# 4. Make the second API call to get the final, natural language answer.\n",
    "second_response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "    messages=messages\n",
    ")\n",
    "print(second_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14e1cd",
   "metadata": {},
   "source": [
    "#### What's Next? Handling Multiple Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49630eb9",
   "metadata": {},
   "source": [
    "So far, our agent has used one tool to answer one question. But a real research workflow is more complex. You might need to ask a question that requires multiple pieces of information from different sources.\n",
    "\n",
    "For example: \"Can you find the title of paper 2305.15334 and also find its citations on Google Scholar?\"\n",
    "\n",
    "The LLM is smart enough to recognize this. Instead of making you go back and forth for each piece of information, it can request all the tools it needs in a single turn. This is called parallel tool calling.\n",
    "\n",
    "When this happens, the response_message.tool_calls object will no longer be a single item, but a list of tool calls. Your code needs to be ready to loop through this list, execute each requested tool, and gather all the results before sending them back to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aece053",
   "metadata": {},
   "source": [
    "#### Example: A Multi-Tool Research Assistant\n",
    "Let's add a second tool to our paper fetcher and see how to handle a parallel request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daeafc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use our get_paper_details function from before.\n",
    "\n",
    "# Here is our new tool. It simulates another research task.\n",
    "def get_citations_for_paper(paper_id: str):\n",
    "    \"\"\"Gets the number of citations for a paper from a mock database.\"\"\"\n",
    "    print(f\"--- TOOL EXECUTED: Getting citations for {paper_id} ---\")\n",
    "    mock_citation_database = {\n",
    "        \"2305.15334\": 121,\n",
    "        \"1706.03762\": 85000,\n",
    "    }\n",
    "    citations = mock_citation_database.get(paper_id, 0)\n",
    "    return json.dumps({\"paper_id\": paper_id, \"citation_count\": citations})\n",
    "\n",
    "# Now, our 'tools' menu has two functions available.\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\", \"function\": {\n",
    "            \"name\": \"get_paper_details\",\n",
    "            \"description\": \"Looks up the title and author of an academic paper given its unique ID.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {\"paper_id\": {\"type\": \"string\"}}, \"required\": [\"paper_id\"]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\", \"function\": {\n",
    "            \"name\": \"get_citations_for_paper\",\n",
    "            \"description\": \"Finds the number of citations for a given paper ID.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {\"paper_id\": {\"type\": \"string\"}}, \"required\": [\"paper_id\"]}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26159c2",
   "metadata": {},
   "source": [
    "Now, watch how we adapt the loop to handle multiple tool calls. The key change is that we now **loop through** response_message.tool_calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30835d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complex prompt that requires BOTH tools.\n",
    "user_prompt = \"Please get me the title of paper 2305.15334 and also find out how many citations it has.\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "# 1. First API call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-8b-instruct:free\", \n",
    "    messages=messages, \n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "response_message = response.choices[0].message\n",
    "messages.append(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebe7ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': None,\n",
      " 'audio': None,\n",
      " 'content': '',\n",
      " 'function_call': None,\n",
      " 'reasoning': None,\n",
      " 'refusal': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"paper_id\":\"2305.15334\"}',\n",
      "                              'name': 'get_paper_details'},\n",
      "                 'id': 'b644eae5-8500-4f09-aa39-8351056c1e4b',\n",
      "                 'index': 0,\n",
      "                 'type': 'function'},\n",
      "                {'function': {'arguments': '{\"paper_id\":\"2305.15334\"}',\n",
      "                              'name': 'get_citations_for_paper'},\n",
      "                 'id': 'f5623cfb-2e95-42b0-8b2e-f204f20ae898',\n",
      "                 'index': 1,\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: print the response to see the tool calls\n",
    "pprint(response_message.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652ca69",
   "metadata": {},
   "source": [
    "We can confirm that it has called both tools as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6173171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM requested 2 tools ---\n",
      "--- TOOL EXECUTED: Searching for paper 2305.15334 ---\n",
      "--- TOOL EXECUTED: Getting citations for 2305.15334 ---\n"
     ]
    }
   ],
   "source": [
    "# 2. Execute ALL requested tools\n",
    "if response_message.tool_calls:\n",
    "    print(f\"--- LLM requested {len(response_message.tool_calls)} tools ---\")\n",
    "    \n",
    "    # This is the key change: We loop through each tool call requested by the model\n",
    "    for tool_call in response_message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        if function_name == \"get_paper_details\":\n",
    "            tool_output = get_paper_details(paper_id=function_args.get(\"paper_id\"))\n",
    "        elif function_name == \"get_citations_for_paper\":\n",
    "            tool_output = get_citations_for_paper(paper_id=function_args.get(\"paper_id\"))\n",
    "        \n",
    "        # We append one message to the history FOR EACH tool call.\n",
    "        messages.append({\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"role\": \"tool\",\n",
    "            \"name\": function_name,\n",
    "            \"content\": tool_output,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90f7893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Complete Message History ---\n",
      "[{'content': 'Please get me the title of paper 2305.15334 and also find out '\n",
      "             'how many citations it has.',\n",
      "  'role': 'user'},\n",
      " ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='b644eae5-8500-4f09-aa39-8351056c1e4b', function=Function(arguments='{\"paper_id\":\"2305.15334\"}', name='get_paper_details'), type='function', index=0), ChatCompletionMessageFunctionToolCall(id='f5623cfb-2e95-42b0-8b2e-f204f20ae898', function=Function(arguments='{\"paper_id\":\"2305.15334\"}', name='get_citations_for_paper'), type='function', index=1)], reasoning=None),\n",
      " {'content': '{\"title\": \"The Role of Social Media in Political Polarization\", '\n",
      "             '\"author\": \"Dr. Eleanor Vance\", \"year\": 2023}',\n",
      "  'name': 'get_paper_details',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'b644eae5-8500-4f09-aa39-8351056c1e4b'},\n",
      " {'content': '{\"paper_id\": \"2305.15334\", \"citation_count\": 121}',\n",
      "  'name': 'get_citations_for_paper',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'f5623cfb-2e95-42b0-8b2e-f204f20ae898'}]\n"
     ]
    }
   ],
   "source": [
    "# Let's verify our message history so far after adding the messages from both tool calls in the cell above\n",
    "print(\"--- Complete Message History ---\")\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feeb25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Second API call for the final answer\n",
    "second_response = client.chat.completions.create(model=\"alibaba/tongyi-deepresearch-30b-a3b:free\", messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c259e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Final LLM Answer ---\\n\n",
      "\n",
      "The paper with the ID **2305.15334** is titled:  \n",
      "**\"The Role of Social Media in Political Polarization\"** by Dr. Eleanor Vance (2023).  \n",
      "\n",
      "It has received **121 citations** so far.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\\\n--- Final LLM Answer ---\\\\n{second_response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e27fc",
   "metadata": {},
   "source": [
    "### 🥊 Grand Finale Challenge: The Narrative Standardization Agent (20 mins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85d230",
   "metadata": {},
   "source": [
    "This is our final task. It will bring together everything you have learned to build a practical research assistant for the exact problem we started with: thematic coding of COVID-19 narratives.\n",
    "\n",
    "##### The Research Context\n",
    "Let's revisit our goal. We are social scientists with thousands of raw, unstructured narratives.\n",
    "\n",
    "A typical raw narrative (Our Input):\n",
    "> \"I was working at the hospital in the South Bronx, and we were running out of everything. It felt hopeless. Every day was a mix of profound sadness for the patients we lost and this incredible solidarity with my fellow nurses. We were all in it together.\"\n",
    "\n",
    "For this data to be useful, we need to process it into a clean, standardized format. This requires a clear \"codebook\" for our analysis and a defined structure for the output.\n",
    "\n",
    "For this challenge we want to identify:\n",
    "- The emotions expressed in each story\n",
    "- Mentions of material conditions (e.g. PPE shortages)\n",
    "- Instances of collective solidarity or isolation\n",
    "- Themes of grief, duty, or burnout\n",
    "- (New) Extract the location in the story and convert it to a location ISO Code For Categorization (South Bronx -> Bronx -> NYC-BX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11284b5e",
   "metadata": {},
   "source": [
    "##### Your Task Part 1: Build the Pydantic \"Codebook\"\n",
    "\n",
    "Before we can build the agent, we must define the structure of our final output. This is your first task. You will create the Pydantic models that will serve as the blueprint for our data.\n",
    "\n",
    "    A) Create the Emotion Codebook\n",
    "\n",
    "In qualitative research, a codebook ensures consistency. An `Enum` is the perfect way to enforce a strict codebook.\n",
    "\n",
    "Your task: Create an `Enum` class called EmotionCode that allows for only the following string values: \"Sadness\", \"Solidarity\", \"Hopelessness\", \"Fear\", and \"Grief\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# --- Your Code Below ---\n",
    "# 1. Define the EmotionCode Enum here.\n",
    "# class EmotionCode(str, Enum):\n",
    "#     SADNESS = \"Sadness\"\n",
    "#     ...\n",
    "\n",
    "class EmotionCode(str, Enum):\n",
    "    SADNESS = \"Sadness\"\n",
    "    SOLIDARITY = \"Solidarity\"\n",
    "    HOPELESSNESS = \"Hopelessness\"\n",
    "    FEAR = \"Fear\"\n",
    "    GRIEF = \"Grief\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a557a3",
   "metadata": {},
   "source": [
    "    B) Create the Final Schema\n",
    "\n",
    "Now, using your EmotionCode enum and the skills you learned in Part 3, define the final data structure.\n",
    "\n",
    "Your task: Create a Pydantic BaseModel called CodedNarrative. It should have the following fields:\n",
    "- summary: A required str.\n",
    "- emotions: A required List of EmotionCode enums.\n",
    "- material_conditions: A required List of str.\n",
    "- primary_borough: A required str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e16691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your Code Below ---\n",
    "# 2. Define the CodedNarrative BaseModel here.\n",
    "# It should use the EmotionCode Enum you created above.\n",
    "class CodedNarrative(BaseModel):\n",
    "    summary: str = Field(..., description=\"A brief summary of the narrative.\")\n",
    "    emotions: List[EmotionCode] = Field(..., description=\"A list of emotions expressed in the narrative.\")\n",
    "    material_conditions: List[str] = Field(..., description=\"Description of the material conditions.\")\n",
    "    primary_borough: str = Field(..., description=\"The primary borough associated with the narrative.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc1e82",
   "metadata": {},
   "source": [
    "##### Your Task Part 2: Build the Agent\n",
    "Now that you have your output schema, you can build the agent that will produce it. I've provided the custom tool for you. Your job is to create the tool's schema and write the full loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- I've provided this tool for you ---\n",
    "def get_borough_iso_code(borough_name: str):\n",
    "    \"\"\"\n",
    "    Takes a standardized NYC borough name and returns its official ISO-like code.\n",
    "    \"\"\"\n",
    "    print(f\"--- TOOL: Getting ISO code for: '{borough_name}' ---\")\n",
    "    # This mock database maps a standardized borough name to a code.\n",
    "    borough_code_database = {\n",
    "        \"Bronx\": \"NYC-BX\",\n",
    "        \"Queens\": \"NYC-QN\",\n",
    "        \"Manhattan\": \"NYC-MN\",\n",
    "        \"Brooklyn\": \"NYC-BK\",\n",
    "        \"Staten Island\": \"NYC-SI\"\n",
    "    }\n",
    "    code = borough_code_database.get(borough_name, \"Invalid Borough\")\n",
    "    return json.dumps({\"iso_code\": code})\n",
    "\n",
    "\n",
    "# 3. DEFINE THE TOOL SCHEMA\n",
    "# Create the 'tools' list with the JSON definition for the get_borough_iso_code function.\n",
    "# Its single parameter should be 'borough_name'.\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_borough_iso_code\",\n",
    "            \"description\": \"Gets the official ISO-like code for a given NYC borough name.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"borough_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The standardized name of the NYC borough.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"borough_name\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BUILD THE AGENT\n",
    "# The raw narrative to be processed\n",
    "raw_narrative = (\n",
    "    \"I was working at the hospital in the South Bronx, and we were running out of everything. It felt hopeless. \"\n",
    "    \"Every day was a mix of profound sadness for the patients we lost and this incredible solidarity with my fellow nurses.\"\n",
    ")\n",
    "\n",
    "# The System Prompt is the \"brain\" of our agent. Note the two-step instruction for location.\n",
    "system_prompt = (\n",
    "    \"You are a social science research assistant. Your task is to thematically code a raw narrative about experiences in NYC during COVID-19. \"\n",
    "    \"First, carefully read the narrative to identify the NYC borough. You must standardize fuzzy locations (e.g., 'South Bronx') into a proper borough name (e.g., 'Bronx'). \"\n",
    "    \"Then, use the `get_borough_iso_code` tool with that standardized borough name. \"\n",
    "    \"Next, analyze the text to determine a one-sentence summary, the emotions expressed (which must conform to the provided EmotionCode enum), \"\n",
    "    \"and any material conditions mentioned. \"\n",
    "    \"Finally, combine all this information into a single JSON object that perfectly matches the `CodedNarrative` schema.\"\n",
    ")\n",
    "\n",
    "# Execute the FULL tool-calling loop.\n",
    "# It should end with a final call using .parse() and your CodedNarrative model to guarantee the output structure.\n",
    "\n",
    "# --- Your Code Below ---\n",
    "# 1. Start the conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": raw_narrative}\n",
    "]\n",
    "\n",
    "# 2. First API Call: The model will reason and ask to use the tool\n",
    "print(\"--- 1. Making first API call to request tool ---\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "messages.append(response_message)\n",
    "\n",
    "# 3. Execute the tool the model requested\n",
    "if response_message.tool_calls:\n",
    "    tool_call = response_message.tool_calls[0]\n",
    "    function_name = tool_call.function.name\n",
    "    \n",
    "    # The model correctly infers that \"South Bronx\" should be \"Bronx\"\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    borough_arg = function_args.get(\"borough_name\")\n",
    "    \n",
    "    # Call our Python function with the argument from the model\n",
    "    function_response = get_borough_iso_code(borough_name=borough_arg)\n",
    "    \n",
    "    # Append the tool's output to the conversation. Hint: you need (tool_call_id, role, name, content)\n",
    "    messages.append({\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"role\": \"user\",\n",
    "        \"name\": \"get_borough_iso_code\",\n",
    "        \"content\": function_response\n",
    "    })\n",
    "\n",
    "# 4. Final API Call: Use .parse() to get the final, validated Pydantic object\n",
    "print(\"--- 2. Making final .parse() call for structured output ---\")\n",
    "final_response = client.chat.completions.parse(\n",
    "    model=\"\",\n",
    "    messages=messages,\n",
    "    response_format=CodedNarrative,\n",
    ")\n",
    "\n",
    "coded_narrative_object = final_response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display the Final, Structured Result ---\n",
    "print(\"--- Final Coded Narrative (Pydantic Object) ---\")\n",
    "pprint(coded_narrative_object.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28710d0f",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "When you run the code, you will see the tool execution messages followed by the final structured JSON object.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"summary\": \"A healthcare worker in the Bronx describes feelings of hopelessness and sadness due to shortages but also experiences incredible solidarity with colleagues.\",\n",
    "  \"emotions\": [\n",
    "    \"Hopelessness\",\n",
    "    \"Sadness\",\n",
    "    \"Solidarity\"\n",
    "  ],\n",
    "  \"material_conditions\": [\n",
    "    \"running out of everything\"\n",
    "  ],\n",
    "  \"borough_iso_code\": \"NYC-BX\"\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a1b7e",
   "metadata": {},
   "source": [
    "---\n",
    "### Final Takeaways: From Chatbot to Research Partner\n",
    "Congratulations on completing this workshop! You've gone from making simple API calls to building a sophisticated, multi-step AI agent capable of performing a real research task.\n",
    "\n",
    "Let's recap the journey:\n",
    "\n",
    "1. **Prompting**: You started by learning that the \"prompt is the program.\" You learned to guide the LLM's behavior with system prompts and improve its reliability with few-shot examples.\n",
    "2. **Structured Output**: You then took control of the model's output. You moved from simply asking for JSON to guaranteeing it with Pydantic models and Enums, turning the LLM into a reliable data processor.\n",
    "3. **Tool Calling**: Finally, you gave the LLM superpowers. By connecting the model's \"brain\" to the \"hands\" of Python functions, you enabled it to interact with custom knowledge bases and standardize messy, real-world data.\n",
    "\n",
    "The most important takeaway is this: an LLM is not just a chatbot. It is a powerful reasoning engine that you can programmatically control and integrate into your research workflow. By combining clear instructions, structured schemas, and custom tools, you can transform it from a simple novelty into a reliable and scalable research partner.\n",
    "\n",
    "The skills you've learned today are the foundation for building any modern AI-powered application. We encourage you to take the code from this workshop, adapt it to your own research questions, and start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b7960",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D_Lab-M2S9YoIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
